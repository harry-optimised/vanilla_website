<!doctype html>

<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>YOLO Version 2 & 3</title>
        <meta name="description" content="The YOLO v2 and v3 architectures and their improvements.">
        <meta name="author" content="Harry Turner">
        <meta name="keywords" content="learn, tutorial, paper, machine learning, deep learning, object detection,
        yolo, yolo v1, loss function, cells, anchors, priors, architecture, feature extraction, classification, yolo v2,
        yolo v3, v2, v3, better, faster, stronger, high resolution, batch normalisation, dimension clusters, k-means,
        fine-grained, multi-scale training, darknet">
        <link rel="icon" type="image/ico" href="../images/favicon.ico">
        <link href="../styles/styles.css" type="text/css" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
        <link rel=“canonical” href=“https://www.harrysprojects.com/articles/rcnn.html” />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../js/Navbar.js" type="text/javascript" defer></script>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-180754451-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-180754451-1');
        </script>
    </head>
    <body>
        <div class="page-container">

            <nav-bar></nav-bar>

            <div>
                <h1>YOLO Version 2 & 3 </h1>
                <p><i>16th January 2021</i></p>
                <h3>Better, Faster, Stronger</h3>
                <p>
                    Both <a href="https://arxiv.org/pdf/1612.08242.pdf">YOLO v2</a> and
                    <a href="https://arxiv.org/pdf/1804.02767.pdf">YOLO v3</a> are improvements on the original YOLO
                    architecture. Both papers are essentially a <i>bag of tricks</i>, in which a variety of start-of-the-art
                    techniques are deployed which each slightly boost the detection of the performance of the network.
                    Although each boost is small (typically on the order of a few percent difference in mAP), in aggregation
                    they add up quick.
                </p>
                <p>
                    Because both papers are just lots of individual improvements on the YOLO architecture, I'm going to
                    go through both at the same time, and the post will be laid out differently
                    to how I normally do it. Specifically, I'm going to start with the YOLO architecture defined in my
                    previous post, and I'll introduce each trick mentioned in both the YOLO v2 and v3 papers and explain
                    how and why it improves the detector.
                </p>
                <p>
                    The YOLO v2 paper is important for another reason, and it's that the authors introduce a new way
                    of training object detectors that leverages classification data. They do this out of recognition
                    that there are many more images with only image classification labels than there are with full
                    bounding box annotations, and it seems a waste not to use them if we can. The technique they introduce
                    harnesses the large amount of classification data available and trains an object detector on both
                    classification and detection data. This increases it's vocabularly and robustness, and results in
                    a detector that can detect over 9000 different object categories by exploiting a hierarchical view
                    of object classification. This technique is very interesting however I choose not to cover it in this
                    post as I am focussing purely on the architectures of object detectors.
                </p>
                <p>
                    For the rest of this post, I'll introduce each trick one by one and talk a bit about how and why
                    it improves the detector. Finally, to save myself having to write YOLO v2 and YOLO v3 every time, for the
                    rest of this post I'll simply refer to v2 and v3 as New YOLO and will refer to YOLO v1 as Original
                    YOLO, I hope this isn't too confusing.
                </p>
                <h3>High Resolution Pretraining</h3>
                <p>
                    In the original YOLO paper, the authors trained the network backbone on the ImageNet classification task
                    at a resolution of <code>224x224</code> but then when they finetuned it on the object detection task,
                    they switched to a resolution of <code>448x448</code>. The reason for the switch is that object
                    detection benefits from larger images, it helps it to find and detect smaller objects. The issue was
                    that when finetuning, the network has to learn how to do object detection, and also adjust to the
                    larger resolution <i>at the same time</i>. So why not just... split the tasks up?
                </p>
                <p>
                    That's exactly what New YOLO does, after they trained the backbone on ImageNet classification task
                    with images of size <code>224x224</code>, they train for 10 epochs on the same image classification
                    task but at a resolution of <code>448x448</code>, then they switch to the object detection task as
                    usual. This simple change gained them almost 4% mAP.
                </p>
                <h3>Batch Normalisation</h3>
                <p>
                    I won't discuss batch normalisation here except to say it's a form of regularisation in neural
                    networks that works really well. Adding it in PyTorch is just a single line of code, and they're not
                    complicated layers, they just involve normalising the weights across the batch. Adding this and
                    removing drop out gained the network 2% mAP. Batch Normalisation is an example of one of those
                    techniques that had widespread positive impact across the whole field of deep learning.
                </p>
                <h3>Dimension Clusters</h3>
                <p>
                    Both SSD and Faster R-CNN use anchors, which are a set of prior boxes with different scales and
                    aspect ratios, each associated with a particular position in the image. These boxes are basically
                    prior beliefs over what kind of objects you expect to see in the image. Actually, you can use this
                    to tailor your object detector to different kinds of problems, in other words inject your domain
                    knowledge into the problem. If you know you're only detecting people then maybe you don't need
                    aspect ratios that define fat and short boxes, because most people are tall and thin (at least
                    the ones I've seen). Recall that in SSD and Faster R-CNN there are 9 anchors in the default
                    implementations, three scales at three different aspect ratios. We could instead use 9 slightly
                    different variations of the tall and thin anchor box and we'd probably get a good detector of people.
                </p>
                <p>
                    Here's a radical idea though: <i>why not learn these anchor box definitions directly from the data?</i>
                    That's exactly that Deep Multi Box does, and what New YOLO now does as well. Specifically they use
                    K-Means clustering to find a set of priors to use at the anchors instead of the manually defined ones.
                </p>
                <p>
                    K-Means is out of scope of this post, however to make it work you need to define the distance between
                    two boxes. The centroids that come out of K-Means will be box representations that we'll use as the anchor
                    priors. One simple way of defining the difference between two boxes is to use the euclidean distance
                    defined over the \( \color{#e83e8c} (x, y, w, h) \) coordinates. But recall from my previous
                    explanations on box parameterisation that this approach is not scale invariant, differences between
                    big boxes will have larger error than differences between small boxes which isn't what we want.
                </p>
                <p>
                    New YOLO therefore adopts the following definition of the distance between two boxes. If you don't
                    know K-Means then just know that this means we're optimising the right thing and the centroid
                    definitions we get out are better as a result of it.
                </p>
                <div class="image-container">
                    <p>\( \color{#e83e8c} d(box, centroid) = 1 - IOU(box, centroid) \)</p>
                </div>
                <p>
                    When using K-Means, you must specify the number of centroids, which is the \( \color{#e83e8c} K \).
                    This also represents the number of anchors we want to use. If we specify \( \color{#e83e8c} K = 1\)
                    then we'll get back a single anchor that happens to represent the average box. Each grid cell will
                    be predicting offsets and scales from that single average box. That's not bad, but obviously we
                    can do better by using more anchors. It turns out that using only five anchors determined through
                    K-Means <i>is as good as using nine manually specified anchors.</i> If we use \( \color{#e83e8c} K = 9 \)
                    we get a very good object detector indeed that was state-of-the-art at the time.
                </p>
                <p>
                    The reason this approach works well is that these specially designed anchors give the network a
                    much better starting representation. Do note however that the anchors are overfit to the types of
                    object you're looking for, and switching to a very different dataset may make the quality of the detections
                    deteriorate quickly if the anchors aren't adjusted and finetuned.
                </p>
                <h3>Fine-Grained Features</h3>
                <p>
                    All object detectors should be able to detect both large and small objects. One way to help them
                    better detect small objects is to provide them finer-grained features, that is, features that appear
                    earlier in the network. Both SSD and Faster R-CNN manage this by running their convolutional detector
                    heads over multiple feature maps, not just the last one, this lets them detect boxes at multiple
                    scales for free. If a feature map is downsized by a factor of 2, then running the detector head on
                    both feature maps is like doing detection at double scale.
                </p>
                <p>
                    New YOLO doesn't do this, and it's not entirely clear from the paper why not. The approach they take
                    instead is to use what they call a passthrough layer. A pass through layer is quite simple, say you
                    have a feature map of size <code>10x10</code> with three channels, and you downsize it to
                    <code>5x5</code> which is your last feature map, and it may have more channels now but let's pretend
                    it still has three. If you wanted to combine the information from both feature maps together, how might
                    you do that? One way to do so would be to notice that for every cell in the <code>5x5</code> feature map,
                    there are four cells in the <code>10x10</code> feature map. You could take those four cells and stack
                    them into third channel. If you did this you'd end up with a feature map of size <code>5x5x12</code>
                    which has the same number of cells as <code>10x10x3</code>. Now that the width and height are the same,
                    you can simply concatenate this feature map with the final <code>5x5x3</code> map you already have to
                    get a feature map of size <code>5x5x15</code>. Finally you run your detector on this augmented feature map.
                    This works because the fine-grained information is encoded into those four new layers. This improvement
                    gained the network 1% mAP. This is much more clearly explained with the image below.
                </p>
                <div class="image-container">
                    <img src="../images/articles/yolov2/fine-grained.png"/>
                    <p><i>An example of two feature maps being combined with the pass through layer.</i></p>
                </div>
                <h3>Convolutions with Anchor Boxes</h3>
                <p>
                    Original YOLO predicts box coordinates with a fully connected layer. SSD and Faster R-CNN, use a convolution
                    instead. See my explanation below for why I believe convolutions are better. New YOLO adopts the new
                    convolution based approach of SSD and Faster R-CNN, and it is virtually identical, the only difference
                    is that whereas Faster R-CNN and SSD predict a set of classes where one of those is the background
                    class, New YOLO instead predicts a set of classes <i>and</i> objectness at the same time. They use
                    this objectness in the same way as original YOLO, it becomes part of the class probability conditional
                    on the box containing an object.
                </p>
                <p>
                    In addition to switching to convolutions, New YOLO picks a very particular image size of <code>416x416</code>
                    such that the final convolution map is size <code>13x13</code>. There's nothing special about 13,
                    what's special is that it's an odd number. This means there is a single cell in the middle of the
                    feature map instead of four. For large objects that dominate the image, they found it more effective
                    to have a single cell in the middle of the feature map be responsible for predicting it, rather than
                    have four cells fight between themselves. This is an example of a lovely trick inspired by good understanding of
                    what is happening deep inside these detectors.
                </p>
                <p>
                    These improvements actually decreased the mAP by about 0.3. It's only a small decrease but it's still
                    a decrease, so why do it? Because it boosted the recall of the detector by 7%, and one of the issues
                    of Original YOLO was it's poor recall. So even though the mAP is reduced, there is now more room
                    for improvement, and that's important.
                </p>
                <h3>Box Parameterisation</h3>
                <p>
                    New YOLO is now using anchor boxes just like Faster R-CNN, but they adopt a new way of parameterising
                    boxes, specifically, a new way of parameteristing the \( \color{#e83e8c} x \) and \( \color{#e83e8c} y \)
                    position of the bounding box, which I'll refer to as \( \color{#e83e8c} b_{x} \) and
                    \( \color{#e83e8c} b_{y} \). First let's remind ourselves of how \( \color{#e83e8c} b_{x} \)
                    and \( \color{#e83e8c} b_{y} \) are parameterised in Faster R-CNN.
                </p>
                <div class="image-container">
                    <p>\( \color{#e83e8c} b_{x} = (t_{x} * a_{w}) - a_{x} \)</p>
                    <p>\( \color{#e83e8c} b_{y} = (t_{y} * a_{h}) - a_{y} \)</p>
                </div>
                <p>
                    In this formula, \( \color{#e83e8c} t_{x} \) and \( \color{#e83e8c} t_{y} \) represent network outputs,
                    and \( \color{#e83e8c} a_{w} \), \( \color{#e83e8c} a_{h} \), \( \color{#e83e8c} a_{x} \), and
                    \( \color{#e83e8c} a_{y} \) are the width, height, and top left position of the anchor
                    box respectively.
                </p>
                <p>
                    The problem here is that \( \color{#e83e8c} b_{x} \) and \( \color{#e83e8c} b_{y} \) are unbounded.
                    To see why, recall that the network outputs are linear in their activations. The network could output
                    a value of a million for \( \color{#e83e8c} t_{x} \) and that would be fine, there's nothing in the
                    architecture that prevents it. Next notice that \( \color{#e83e8c} b_{x} \) is computed by
                    multiplying \( \color{#e83e8c} t_{x} \) by \( \color{#e83e8c} a_{w} \) which is constant.
                    Because \( \color{#e83e8c} t_{x} \) is unbounded, the network can make \( \color{#e83e8c} b_{x} \)
                    whatever it wants by predicting the appropriate value of \( \color{#e83e8c} t_{x} \). This doesn't
                    really make sense, the top left cell shouldn't be able to predict a box in the bottom right corner.
                    We have a poor representation. In practice this manifests as unstable training (or at least not as
                    stable as it could be).
                </p>
                <p>
                    New YOLO fixes this by adopting a new parameterisation for \( \color{#e83e8c} b_{x} \) and
                    \( \color{#e83e8c} b_{y} \).
                </p>
                <div class="image-container">
                    <p>\( \color{#e83e8c} b_{x} = \sigma(t_{x}) + a_{x} \)</p>
                    <p>\( \color{#e83e8c} b_{y} = \sigma(t_{y}) + a_{y} \)</p>
                </div>
                <p>
                    This is much better because \( \color{#e83e8c} t_{x} \) is now bounded to fall between 0 and 1
                    because of the sigmoid function. I want to clarify something that tripped me up when I first tried
                    to understand this. The equation presented above makes it sound like the value for
                    \( \color{#e83e8c} b_{x} \) is equal to the x-coordinate of the anchor box plus or minus 1.
                    What you have to remember is that everything is normalised, so 1 in this case actually means the
                    width of the anchor box. In other words, if the value of \( \color{#e83e8c} \sigma(t_{x}) \) is
                    1 then \( \color{#e83e8c} b_{x} \) will be equal to \( \color{#e83e8c} t_{x} + a_{w} \).
                    This means that the network can <i>only</i> shift boxes a short distance <i>around</i> the
                    original anchor position no matter what \( \color{#e83e8c} t_{x} \) takes. This makes much more
                    sense and makes training more stable.
                </p>
                <h3>Class Prediction</h3>
                <p>
                    Whilst we're on the topic of network outputs and representations, let's revisit the class outputs.
                    Recall that Original YOLO predicts classes for <i>each grid cell</i> and each grid cell predicts two boxes that
                    are constrained to have the same class. New YOLO changes this. Classes are now predicted <i>per anchor box</i>
                    associated with each grid cell. So if a grid cell predicts five boxes, each box can have a different
                    class. This decouples the class prediction from the location, which makes sense. To see why, imagine
                    that a single grid cell predicts two boxes, one is offset to the left, the other is offset to the right.
                    Even though they both come from a similar starting location, they may end up quite far from each other, and
                    looking at different parts of the image so why should they be constrained to have the same class?
                </p>
                <p>
                    In addition to having each anchor box predict it's own class, New YOLO replaces the softmax distribution
                    over classes with a set of logistic classifiers. This is because sometimes a single object can belong
                    to more than one class. If you've got a class for Woman and class for Person, then forcing the network
                    to choose between them is confusing and doesn't make sense, but this is exactly what softmax will
                    try and do because it tries to enforce a probability distribution. A set of logistic classifiers
                    lets the network output high probability for both Person and Woman in this case, which is what we want.
                </p>
                <h3>Multi Scale Training</h3>
                <p>
                    As well as the fine-grained features defined earlier, there is another way to help the network better detect
                    objects at a variety of scales, and that is to force it to learn them through brute force, by
                    using many different scales of images throughout training. If we pass an image of a car through the
                    network, the network learns to detect cars at that scale, if we shrink the image by a factor of two
                    and do it again, the network learns to detect cars at half scale, and so on. Theoretically, this is
                    all there is to multi scale training, the trick is how to do it efficiently.
                </p>
                <p>
                    Notice that the network with the new anchors is fully convolutional, there are no fixed size fully
                    connected layers, so we can put any size image through the network as long as the convolution sizes
                    work out. It turns out the network downsizes images by a factor of 32, so as long as we pick an image
                    size that is a factor of 32, we can pass it through the network with no issue. We don't want to be swapping
                    the image size on every single pass through the network though (also images within a single batch must
                    all be the same size), so a good compromise is to train the network for 10 batches and then swap to a random
                    image size that is a factor of 32, then train for another 10 batches, and so on.
                </p>
                <p>
                    This works really well, and not only does the network learn to predict objects at different scales but
                    it learns good object detection whether you're using small images or large images. Using small images
                    has an advantage, it's fast! This means that once we have a trained network, we can choose high accuracy
                    or high speed by picking the image size we want to use. That's a very useful knob to be able to tune.
                </p>
                <h3>Upgrade the Backbone</h3>
                <p>
                    Last but not least, New YOLO uses a new backbone, which the authors call Darknet-53 because it has
                    53 layers. YOLO v2 introduced DarkNet-19 and then YOLO v3 introduced innovations from ResNets to
                    allow it to go deeper. I'll just blend the contributions from both papers here and introduce the
                    DarkNet-53 architecture directly.
                </p>
                <p>
                    My attitude towards backbones is that they are mostly interchangeable and therefore
                    improvements in the backbone are mostly orthogonal to improvements in object detection in general. I'll cover
                    some of the Darknet-53 improvements here because I think they're interesting, but note that you
                    could swap out Darknet with any backbone, whether it's ResNet or EfficientNet, or one of your own
                    design. There are innovations in backbone design coming out all the time and with only a few
                    changes it would be possible to make the New YOLO detector head work with those as well.
                </p>
                <p>
                    Here is DarkNet-53:
                </p>
                <div class="image-container">
                    <img src="../images/articles/yolov2/darknet.png"/>
                    <p><i>DarkNet-53 from the YOLO v3 Paper, Credit to Joseph Redmon.</i></p>
                </div>
                <p>
                    DarkNet builds off lots of recent innovations in network design, such as using lots of <code>3x3</code>
                    filters and doubling the number of channels after every pooling step. They follow the work on Network
                    in Network and use global average pooling to predictions as well as <code>1x1</code> filters between
                    <code>3x3</code>. They also use shortcut identity connections much like those used in the ResNet
                    architectures. The final model has 53 convolutional layers.
                </p>
                <p>
                    Just as in Original YOLO, they train the backbone on the image classification task on standard <code>224x224</code>
                    and then train for a few epochs on the higher resolution as discussed above. Then the swap to detection
                    by removing the last convolutional layer and on three <code>3x3</code> convolutional layers with 1024
                    filters each followed by the detector head which is a <code>1x1</code> filter with the number of outputs
                    equal to the number of outputs they need for detection. Each predicted box has four coordinates, an objectness
                    score, and a distribution over classes. For 20 classes, each box therefore has 25 outputs. Recall that
                    each cell is associated with \( \color{#e83e8c} K \) anchors, so for 5 anchors, the number of outputs required from the final
                    <code>1x1</code> convolution is \( \color{#e83e8c} K(20 + 4 + 1) = 125\). See how simple the actual
                    object detector head is in this entire network, it all boils down to a single <code>1x1</code>
                    convolution with 125 outputs.
                </p>
                <h3>Wrap Up</h3>
                <p>
                    I want to remind you that although YOLO v2 and v3 appear to be new architectures/object detectors,
                    they're actually just a bag of tweaks that make object detectors better. This is why it's they're important
                    papers to pay attention to. Almost all of these tricks are orthognal to each other could be applied
                    to other detector architectures. The most important take away to get from this particular article is
                    not which exact set of tricks are found in YOLO v2 or YOLO v3, but to realise how and why each of
                    these tricks works to make object detection better in general.
                </p>
                <br />
                <div id="disqus_thread"></div>
                <script>
                    var disqus_config = function () {
                    this.page.url = "https://www.harrysprojects.com/blogs/yolov23.html";
                    this.page.identifier = "yolov23";
                    };

                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://harrysprojects-com.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })();
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

                <br />
                <br />
            </div>

        </div>
    </body>
    <footer>
    </footer>
</html>