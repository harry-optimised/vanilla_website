<!doctype html>

<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Harry's Projects</title>
        <meta name="description" content="">
        <link rel="icon" type="image/ico" href="../images/favicon.ico">
        <link href="../styles/styles.css" type="text/css" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
        <link rel=“canonical” href=“https://www.harrysprojects.com/articles/spp.html” />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="../js/Navbar.js" type="text/javascript" defer></script>
    </head>
    <body>
        <div class="page-container">

            <nav-bar></nav-bar>

            <div>
                <h1>Spatial Pyramid Pooling.</h1>
                <p><i>20th September 2020</i></p>
                <h3>Spatial Pyramid Pooling</h3>
                <p>
                    Shortly after R-CNN was released, came <a href="https://arxiv.org/abs/1406.4729">SPP-Net</a>, which
                    demonstrated what I refer to as a generally applicable result, something that could be applied
                    to many different networks in object detection, classification, segmentation, etc, and provide significant
                    improvements in all. These papers are usually well worth reading.
                </p>
                <p>
                    Their insight was that up to this point, all these tasks used convolutional
                    networks with fully connected layers at the end, and that these fully connected layers enforced a
                    fixed size constraint on the input image. In other words, if the first fully connected layer went from <code>A</code>
                    inputs to <code>B</code> outputs, then
                    the <i>flattened and resized</i> output from the last convolutional layer <i>must</i> be of size <code>A</code> (otherwise the matrix computation wouldn't line up), which fixes the next
                    convolutional layer, and the next, all the way to the input. This is why AlexNet needs an input of <code>227 x 227</code>.
                    SPP-Net introduced a new
                    layer between the final convolutional layer and the fully connected layer that removes this constraint.
                </p>
                <p>
                    It's called a spatial pyramid pooling layer and here's how it works. Refer to the image below,
                    I'll explain it from the bottom of the image up.
                    Once you've passed your
                    image through the convolutional layers, you get a set of feature maps, for example, let's say we get 256 feature maps. You max
                    pool each feature map to produce a vector of length 256 (remember max pool picks the largest value
                    in each feature map and there are 256 maps). So you've now got a vector of 256. Technically, this vector
                    represents a summarised version of our feature maps, you could pass that vector on to the final classification layer.
                    In other words, the SPP layer has received a set of convolutional feature maps as input and flattened them into a
                    256 length vector. But we can do better.
                </p>
                <p>
                    We'll do the same thing again, but this time you split
                    each feature map into four quadrants (bins) and do a max pool in each bin, this gives you four
                    numbers for each of the 256 feature maps, or four lots of 256-length vectors, then you do it again
                    but split each feature map into 16 bins which gives you 16 lots of 256-length vectors. Now lets
                    stop there and stack all those vectors up, your final vector is made up of stacking all those 256-length
                    vectors, the 16 from the lowest level plus the 4 from the next level plus the global one to produce
                    21 lots of 256-length vectors, or a single vector of length 5376. It is this final vector that we end up
                    passing on to the later layers in the network.
                </p>
                <p>
                    Make sure you understand the significance, work this through in your mind: <i>no matter what size the input is</i>,
                    it will always produce a vector of length 5376 (for that particular configuration of bins).
                </p>
                <div class="image-container">
                    <img src="../images/articles/spp/spp-working.png"/>
                    <p><i>This is Figure 3 from the SPP-Net paper, credit goes to the paper authors.</i></p>
                </div>
                <p>
                    Why is this relevant to object detection? Most of the SPP-Net paper is about the authors applying their
                    new idea to lots of different domains and showing how it improves performance in all of them, including
                    R-CNN! They make a very important observation, they recognise that the speed bottle neck in
                    R-CNN is because the CNN is applied to every <i>image</i> region (all 2000), meaning it is run 2000 times.
                    But they realised that they could run the CNN feature extractor <i>once</i> on the entire image to produce a set of feature
                    maps, and <i>then</i> apply their
                    SPP-Pooling layer to each region.
                </p>
                <p>
                    Let me repeat that again, R-CNN applies the CNN to every <i>image region</i> in the original image. But
                    SPP-Net applies the CNN once to produce the feature maps, and then applies
                    <i> only the SPP layer and fully connected layers to each feature region.</i>
                    This process is illustrated in the image below.
                </p>
                <div class="image-container">
                    <img src="../images/articles/spp/spp-feature-maps.png"/>
                    <p><i>This is Figure 5 from the SPP-Net paper, credit goes to the paper authors.</i></p>
                </div>
                <p>
                    Why is this important? Two reasons, the first is that by doing a spatial pyramid pool of the region, rather
                    than simply flattening it, you
                    get a better feature vector because it's considering multiple scales. Second, by running the pooling
                    on feature map regions, instead of image regions, you only run the CNN once, which makes your
                    network about 2000 times faster, not bad. (In practice, for various reasons, they saw a speed up of about 200).
                </p>
                <h3>Next Up</h3>
                <p>
                    With that, we're ready to tackle Fast R-CNN. See you in the next post.
                </p>
                <p><a href="https://www.harrysprojects.com/articles/fastrcnn.html"/>Next: Fast R-CNN</a></p>
                <br />
                <br />
            </div>

        </div>
    </body>
    <footer>
    </footer>
</html>