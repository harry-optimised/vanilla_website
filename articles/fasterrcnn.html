<!doctype html>

<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Faster R-CNN</title>
        <meta name="description" content="How the faster r-cnn deep object detector works.">
        <meta name="author" content="Harry Turner">
        <meta name="keywords" content="deep learning, object detection, faster r-cnn, architecture, region proposal network, anchors, dense boxes, training, machine learning">
        <link rel="icon" type="image/ico" href="../images/favicon.ico">
        <link href="../styles/styles.css" type="text/css" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
        <link rel=“canonical” href=“https://www.harrysprojects.com/articles/fasterrcnn.html” />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="../js/Navbar.js" type="text/javascript" defer></script>
    </head>
    <body>
        <div class="page-container">

            <nav-bar></nav-bar>

            <div>
                <h1>Faster R-CNN.</h1>
                <p><i>11th October 2020</i></p>
                <h3>Faster R-CNN</h3>
                <p>
                    Faster R-CNN is a powerful, modern two-stage object detector still in use today. As well as that, it
                    provides a framework for understanding some of the innovations coming from Facebook AI Research, namely
                    Mask R-CNN, Panoptic Segmentation, and Mesh R-CNN. It's well worth spending some time to understand this
                    model.
                </p>
                <p>
                    If you've not done so already, I suggest you read the previous post on <a href="https://www.harrysprojects.com/articles/fastrcnn.html">Fast R-CNN</a>, as this post builds
                    on that one and generalises all those concepts to the family of two stage object detectors.
                    Faster R-CNN is about as complex as it gets for now, later papers add things here and there but the details
                    are in the additions, if you
                    can understand the framework that Faster R-CNN provides, it'll really help to organise your thinking. I
                    spend the first part of this post presenting the architectures of a few modern object detectors in
                    terms of a framework of building blocks, it might get a bit repetitive but it's an important point so please
                    bear with me.
                </p>
                <p>
                    Also note that I flip back and forth between discussing Fast R-CNN and <i>Faster R-CNN</i>, I'm being
                    very specific in my terminology, it's a shame the names are so similar.
                </p>
                <h3>How to Think About It</h3>
                <p>
                    Let's start by analysing Fast R-CNN in terms of building blocks. We have a block for region proposal, which
                    is Selective Search, a block for extracting features of those proposals, (a CNN backbone), a block for classifying
                    proposals, and a block for finetuning the boxes of those proposals, both of which are branches of a detector head.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/blocks_fastrcnn.png"/>
                    <p><i>The building blocks of Fast R-CNN.</i></p>
                </div>
                <p>
                    Now we can swap in and out different blocks, we could use SIFT features instead of a CNN, or SVMs instead
                    of the classification branch. But neither of those improve on Fast R-CNN. We could use the new (as of 2014) proposal
                    mechanism called Edge Boxes instead of Selective Search, it doesn't produce better boxes but it's faster.
                    Or we could use dense boxes instead. What are dense boxes? I'll cover them in more detail when we talk about
                    anchors, but essentially you propose a box at <i>every</i> position in the image, at a range of scales and
                    aspect ratios. Contrast this to Selective Search which tries to only propose boxes that might contain something.
                </p>
                <p>
                    The point I'm making in this section is that you can think of a large number of object detectors as being made up of
                    these components.
                </p>
                <p>
                    Fast R-CNN is Selective Search + backbone + two small networks for regression and classification, cleverly
                    tied together to enable end to end training. If you use dense boxes instead, you end up with a one
                    stage network that is <i>similar</i> to a detector called OverFeat. (The reason it is suddenly called one-stage,
                    is because there is practically no computation for dense boxes, they are predefined). The performance difference will be down to whether
                    dense boxes recall more objects than Selective Search. It becomes really useful to think about these detectors in this way.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/blocks_overfeat.png"/>
                    <p><i>The building blocks of R-CNN and OverFeat.</i></p>
                </div>
                <p>
                    Now.. what is a region proposal mechanism really..? It's something that proposes regions that hopefully
                    contain objects. So it's something that.. detects objects? Isn't our region proposal mechanism at heart just
                    another object detector?? Yes, and so we should be able to do it with a network!
                </p>
                <p>
                    Let's swap the Selective Search building block out for a neural network based proposal mechanism, let's
                    call it a <i>Region Proposal Network</i>.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/blocks_fasterrcnn.png"/>
                    <p><i>The building blocks of Faster R-CNN.</i></p>
                </div>
                <p>
                    It turns out the Faster R-CNN it's basically two objects detectors back to back, in a cascade, welcome to
                    the king of two-stage detectors. As I said a few lines ago, it's useful to think about it as
                    <i> replacing Selective Search</i> with a neural network. Not only does this turn out to be faster, but it
                    proposes much better regions as we'll see. The rest of the network is just normal Fast R-CNN, so the same
                    backbone, the same classifier head and same regressor head. Thinking about networks in this way is simple
                    but powerful, I hope you see what I'm getting at.
                </p>
                <h3>The Architecture</h3>
                <p>
                    I have a confession to make, the diagram of Faster R-CNN above is a lie, kind of. Although there
                    are two networks, they're not separated quite so cleanly, I said it's useful to
                    <i> think about the network like that</i> but it is implemented in a more efficient way. Specifically, we
                    are going to end up with two object detectors, but they're going to share certain building blocks for
                    efficiency. Therefore it will be useful to define some specific terminology to allow us to be precise. This will
                    be largely consistent with the literature, although the literature is not exactly consistent either.
                </p>
                <p>
                    We have two networks, the first will propose regions, and it's called the <i>Region Proposal Network</i>, the
                    second will detect objects, we'll call it the <i>Detector Network</i>. In a previous post, I said you could
                    think about object detectors as having backbones, and heads. So we have a <i>Region Proposal Network Backbone</i>,
                    a <i>Region Proposal Network Head</i>, a <i>Detector Backbone</i>, and a <i>Detector Head</i>. The job of the
                    Region Proposal Network Head is to predict region proposals that contain <i>any</i> object, the job of the Detector
                    Head is to classify those proposals and finetune them into actual object bounding box predictions.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/terminology.png"/>
                    <p><i>Faster R-CNN is made up of a Region Proposal Network and a Detector Network.</i></p>
                </div>
                <p>
                    Now we come to our first implementation detail, <i>both networks share the same backbone. </i>
                    We can do this because
                    both networks are doing object detection on the same image, and features useful for predicting proposals are probably
                    also useful for classifying and regressing those proposals. From now on, I'll just call it <i>the backbone</i>.
                    So currently our architecture looks like this.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/combined_backbone.png"/>
                    <p><i>The Region Proposal Network and Detector share the same backbone.</i></p>
                </div>
                <p>
                    Remember how the Fast R-CNN head received region proposals and feature maps, and it first used an ROI Pooling
                    layer to warp all projected feature regions into a fixed length vector before doing classification and regression.
                    Well all those pieces still exist here, so <i>you already know how the Detector Head works, it's just the Fast R-CNN head.</i>
                </p>
                <p>
                    Likewise, the backbone is exactly the same as in Fast R-CNN as well, it produces features that are consumed by
                    both the Region Proposal Head and the Detector Head. The new part is the Region Proposal
                    Head, which we'll look at in a minute.
                </p>
                <p>
                    There's still a missing piece, if the Region Proposal Network (or <i>RPN</i> as I'll start to call it) is
                    proposing regions for the Detector Network,
                    then <i>what is proposing regions for the Region Proposal Network?</i> You read that right, the RPN is an
                    object detector now, and all the object detectors we've looked at so far used modules such as Selective Search,
                    it can't just magic proposals from thin air. <i>For now, let's just give it
                    dense proposals.</i> It's not quite as simple as that, we'll come back to it shortly and develop the
                    idea into anchors which is how it actually works.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/with_dense_proposals.png"/>
                    <p><i>The Region Proposal Network get's it's proposals from something
                called Anchors, which are a (bit) like dense proposals.</i></p>
                </div>
                <p>
                    And so finally we're ready to explore the details. The <i>architecture</i> presented directly above
                    is missing a few details but it's enough for the following discussions. The RPN starts with a
                    <i> dense box-like process</i> and predicts region proposals, using a CNN backbone to extract the features
                    from those dense boxes. The Detector also uses those features, but uses the region proposals from the RPN
                    instead. From that point, it's just Fast R-CNN doing it's normal thing of classifying into classes and finetuning.
                </p>
                <p>
                    From this perspective, you can see that the RPN is just a clever way of turning naive dense boxes into much better region
                    proposals, that outperforms Selective Search. That's <i>basically</i> all that Faster R-CNN is, plus a few tricks which we'll
                    see shortly.
                </p>
                <p>
                    Coming up, we'll address those dense boxes and develop the idea of <i>Anchors</i> which is how it actually works,
                    we'll look at the RPN Head and how it's just a variant of the Detector Head, and we'll discuss
                    a little bit about how all the pieces are wired together.
                </p>
                <h3>Dense Boxes</h3>
                <p>
                    The RPN Head needs to get it's box proposals from <i>somewhere</i>, it's still an object
                    detector. I said previously that we'd supply it dense boxes, let's look a bit more closely at what those are.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/anchors.png"/>
                    <p><i>Dense Box Proposals</i></p>
                </div>
                <p>
                    Look at the image above, our goal is to detect the dog. Selective Search would
                    propose regions that likely contained objects, hopefully one of them would be the dog, and our network
                    head would classify and finetune it. So with dense boxes, we need to be able to propose boxes that at least
                    get close to containing the dog. But the dog could also be at any position, so we need to be invariant to
                    location as well. We can do this by sampling a regular grid of boxes over the entire image.
                </p>
                <p>
                    Start with the top left image, I divided it into squares, each square is a region proposal. If you count
                    them all there are 35 region proposals, that's nothing compared to the 2000 we get from Selective Search.
                    We need more boxes, and we need them to be more finegrained, so a natural next step would be to let them
                    overlap and sample more frequently. This is shown in the top right image, although it's getting a bit messy now.
                    Let's say we doubled our sampling frequency in each direction so we've now got about 3 times the number of boxes, we're up to 100.
                    (Don't bother counting them all).
                </p>
                <p>
                    Each region proposal so far is still quite small, none of the are really capable of detecting the dog. Even
                    the one right in the middle of the dogs face would have to be regressed several times larger than itself to
                    fit. Let's add more proposals. For <i>every proposal we already have</i>, we're going to add two more
                    at the same position, one at about double scale and one about triple scale. In the bottom left image I've shown this for one
                    square in red because it starts getting very messy but if you imagine we do that for every box proposal, we're up
                    to 300 proposals.
                </p>
                <p>
                    You can probably see where this is going now, the last step is to add a few different aspect ratios, so, in
                    <i> addition</i> to the different sizes we already have, for <i>each</i> original white box proposal we have
                    nine now, three sizes at three different aspect ratios. I have shown height bigger than width to illustrate,
                    but the same applies to width bigger than height.
                    We're at 900 region proposals.
                </p>
                <p>
                    These dense boxes are incredibly quick to compute, notice how we've predefined them, it doesn't matter what
                    the image is, we always sample the same way. This can be done with a few lines of numpy code, no slow Selective Search or Edge Boxes.
                    We call them dense because they densely cover the entire image, simple.
                </p>
                <h3>Anchors</h3>
                <p>
                    If you understand dense boxes, you can understand anchors too. The key piece of intuition is that we don't sample
                    a regular grid of boxes on the <i>image</i> and send those to the RPN Head, <i>instead</i>,
                    we sample a regular grid of boxes <i>from the last convolutional map.</i> The best illustration of this is
                    from the paper itself, let me copy it in below.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/anchors_paper.png"/>
                    <p><i>Figure 1 from the Faster R-CNN paper, credit to Shaoqing Ren, Kaiming He, Ross Girshick and Jian Sun.</i></p>
                </div>
                <p>
                    Look at the convolutional map, doesn't it look just like a regular grid of boxes? We don't even need to sample,
                    we can just take <i>every position</i> in the feature map as a dense box proposal. A typical backbone might
                    give us a feature map of size 15x15 (depending on the backbone), so that's 225 proposals right there. If
                    we do the scale and aspect ratio trick, we get about 2000 proposals.
                </p>
                <p>
                    Each position in the feature map has an <a href="https://blog.christianperone.com/2017/11/the-effective-receptive-field-on-cnns/">effective receptive field </a>
                     that covers quite a large patch of the original image, so it's <i>as if we had a whole load of overlapping
                    box proposals on the original image</i>, except we can do it all on the feature map instead.
                </p>
                <p>
                    So just to recap where we are, we've got a feature map of size 15x15, each of the 225 positions in that
                    feature map are our box proposals, and for each one we use 3 different scales and 3 different aspect ratios to
                    get about 2000 box proposals. <i>These special box proposals coming from the feature map are called anchors.</i>
                </p>
                <p>
                    In Fast R-CNN, we discussed how the network head turns region proposals into fixed length vectors by passing
                    them through a special layer called the Region Pooling Layer. <i>We don't need to do this for anchors</i>.
                    Because they are defined on the feature map, we can do something clever with convolutions. I'll come back to this
                    in a minute when I talk about the RPN Head, but for now, know that we can go straight from feature map
                    to predicted boxes with no Region Pooling step.
                </p>
                <p>
                    The last piece of the puzzle is to understand how anchor boxes are parameterised at runtime. In other words, how
                    do you go from anchor box to predicted box on the final image? Recall that Fast R-CNN
                    outputs four numbers for each box, but these four numbers are parameterised scales and offsets for each
                    box proposal, I discussed this in detail at the end of the Fast R-CNN post. To be even more explicit,
                    the box proposal from Selective Search is in actual pixel coordinates, but is shifted and scaled by the
                    four outputs from the network in a special way that makes sure the box doesn't get turned inside out. See the
                    end of the Fast R-CNN post if that didn't make sense.
                </p>
                <p>
                    Firstly, we must figure out what pixel coordinates our anchors correspond to in the actual image. Fortunately
                    we can do this quite easily by mapping the size of the feature map onto the size of the image. For example,
                    in our 15x15 feature map example, if the image was 150x150 pixels then each anchor is <i>positioned </i>
                    every 10 pixels in the actual image. How big is each anchor in the original image? Well we get to choose. Because
                    we have 3 different scales for our boxes, we might define one at 10x10, one at 30x30, and one at 50x50, but it's
                    entirely up to you. The important thing is that for <i>every anchor defined on your feature map</i>, there is
                    an equivalent set of corresponding <i>pixel coordinates in the original image.</i>
                </p>
                <p>
                    Secondly, once we have the four outputs from the network, we can use the parameterisation equations from
                    the R-CNN paper to <i>convert</i> the anchor pixel coordinates (which we know) into our predicted box pixel coordinates. I
                    have repeated these equations below, the <code>dx, dy, dw, dh </code> are the four outputs from the network, <code>Ax, Ay, Aw, Ah </code>
                    are the anchor coordinates in pixels.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/anchor_targets.gif"/>
                    <p><i>Equations for computing box coordinates from anchors and network outputs.</i></p>
                </div>
                <p>
                    That's how the anchors work at runtime, there is a section in the paper called <i>A Loss Function for Learning Region Proposals </i>
                    that describes how this process works during training as well. I made a conscious decision not to cover it here because
                    this post is already getting long, however if you've been following along then you have all the bits
                    you need to understand it. The trick is that for training you also have to parameterise the anchors with respect to a ground truth box,
                    (think of this as: what scaling and offset transformation would I have to apply to this anchor in order to warp it into
                    the ground truth box). Those four parameterised numbers are what you train the network to output.
                </p>
                <h3>The RPN Head</h3>
                <p>
                    The RPN Head turns anchors into actual box proposals in pixel coordinates, because that's
                    what the Fast R-CNN detector head expects. But as I said earlier, you don't need to warp anchors using
                    a Region Pooling layer, you can do it all with a clever convolution.
                </p>
                <p>
                    If you read the section in the paper that describes anchors, you'll find that they initially define it as sliding a small
                    network over the feature map, they then state that this is naturally implemented by a convolutional layer.
                    This is what they mean, first take a look at this image again.
                </p>
                <div class="image-container">
                    <img src="../images/articles/fasterrcnn/anchors_paper.png"/>
                    <p><i>Figure 1 from the Faster R-CNN paper, credit to Shaoqing Ren, Kaiming He, Ross Girshick and Jian Sun.</i></p>
                </div>
                <p>
                    We're already starting at a feature map, I'm going to call it the semantic feature map because although
                    the diagram shows just one channel for easier visualisation, it actually has lots of channels, lets say 512, each hopefully
                    encoding for specific meaningful features. If we do a convolution on this feature map, what
                    do we get? Well, another feature map, but lets say our convolution downsizes from 512 channels to just 4 in our
                    output feature map. Is it possible we could interpret each of those four channels as the four regression outputs
                    of the network? Well yes definitely, the maths will work, and as long as we train the outputs to match
                    the parameterised targets we came up with earlier, this will actually work. This is sort of similar to
                    what Fast R-CNN is doing <i>except we're doing it with convolutions instead.</i>
                </p>
                <p>
                    But we don't just need regression outputs, we need class scores as well. For the RPN Head, we
                    actually have just two class scores, object, and not object, but we still need a layer for this. Here's
                    what actually happens. We start with our semantic feature map of 512 channels, we first do a convolution to
                    downsize from 512 to <i>256</i> channels, let's call this the <i>intermediate layer</i> (note that the height and width of the feature map remain the same throughout this whole process). Then, we do a
                    <i> 1x1 convolution</i> on the intermediate layer to downsize it from 256 to 4, this produces <i>another feature map </i>
                    that we interpret as our regression outputs. Then, we do another convolution <i>on the same intermediate layer</i>,
                    which produces a fourth feature map with just 2 channels, the classes. So both the regression and class feature maps are
                    produced <i>from the intermediate layer</i>. This is how we branch in the world of convolutions, we do it with 1x1s.
                    It's unfortuneate that the diagram above makes it look like fully connected layers.
                </p>
                <p>
                    Hopefully that diagram now makes sense to you. Each position in the feature map has K anchors associated with it,
                    K being 9 in this case, for all those scales and aspect ratios. They do a convolution to produce the intermediate
                    feature map which has 256 channels, then
                    they do two 1x1 convolutions to produce the output branches, with 2K scores and 4K coordinates respectively. Why 2K? 2 classes
                    for every K box <i>for that specific position.</i> 2K scores and 4K coordinates are predicted <i>for every single position
                    in the feature map.</i>
                </p>
                <p>
                    That's the Region Proposal Head, a convolution layer followed by two sibling 1x1 convolution layers.
                </p>
                <p>
                    You're probably getting fed up of all this repeition, but I personally struggled with anchors. It was a long time before
                    they finally clicked and this part is what tripped me up.
                </p>
                <h3>A Few Bits and Pieces</h3>
                <p>
                    Because of all the anchors, the RPN outputs <i>a lot</i> of boxes. But not all of them are useful. Therefore
                    there is a Non Maximum Suppression (NMS) step <i>in between</i> the output of the RPN and the input to the
                    Detector Head. The threshold is set to 0.7, which ends up filtering the number of boxes down to about 2000 (same as Selective Search),
                    by keeping the regions with the highest objectness score. This doesn't harm the accuracy of the detector,
                    it just reduces the number of proposals it has to process.
                </p>
                <p>
                    Why bother with two stage at all, it sounds like the RPN could be adapated to just do the whole thing by giving
                    it more than two classes to predict. Well remember at the beginning when I talked about building blocks that
                    this is basically what OverFeat is, and indeed this is the
                    bridge to one stage detectors. The authors defend their decision for two stages by describing it as a sort
                    of attention mechnism, the detector attends to the proposals. There is an advantage to this in that it quickly
                    removes a lot of false positives, something that one stage detectors like YOLO struggle with. It is however,
                    comparatively slower.
                </p>
                <h3>Training</h3>
                <p>
                    I said above that I'm not going to cover the training of Faster R-CNN, because it's already quite a
                    heavy going post, and you already have the bits you need to understand it anyway. We touched on training
                    of Fast R-CNN in the last post and the RPN is trained in a similar way.
                </p>
                <p>
                    With that said however, I do want to point out one detail, and that's that we're back into the regime of
                    multi-stage training. Specifically, we first train the RPN, then we train a separate Fast R-CNN with those
                    proposals, then we use those weights to initialise the RPN (again), then finetune the RPN and then finetune
                    the Fast R-CNN head (again).
                </p>
                <p>
                    If that didn't make sense, don't worry, it wasn't meant to. It's all a bit messy and it's a hard thing to
                    describe in words, in fact the paper does a good job of explaining it so have a look there if you are interested. No,
                    all I wanted to point out is that we're back to multistage training again because of the complexity of having another
                    network you need to train.
                </p>
                <p>
                    What I think <i>is</i> interesting is to consider why you need this multistage regime, what would happen
                    if you just trained it end to end (which is the trend nowadays). They speculate on the reason in the paper and
                    then leave it to future work, but the gist is that your Fast R-CNN head relies on (good) proposals from the
                    region proposal mechanism, otherwise it cannot be trained at all, but early in training the RPN is probably not
                    generating very good proposals. The distribution of proposals going into the Fast R-CNN head <i>is constantly moving</i>,
                    which I assume makes it very hard for the Fast R-CNN head to learn anything as it can't rely on it's inputs!
                </p>
                <p>
                    I expect this makes the training unstable, and you'd have to wait for the RPN to be trained to the point where
                    it's generating half decent proposals before the Fast R-CNN head began to improve. Anyway, that's all I wanted
                    to mention about that, mostly I thought it was insightful.
                </p>
                <h3>And We're Done</h3>
                <p>
                    That's it for Faster R-CNN. What I hoped to do in this post was build up a bit of a framework for thinking
                    about object detection architectures. Faster R-CNN is still in active use today, modern versions of it
                    are slightly augmented versions that improve on it in various ways, we'll see those improvements soon. We'll also
                    see the evolution of Faster R-CNN into other vision tasks like mask prediction and even 2D to 3D prediction. The good news is,
                    if you followed these object detection posts so far, you'll find these next few papers natural extensions of what we've already discussed and should
                    be able to fit them in quite nicely into what you already know and understand.
                </p>
                <p>
                    Onwards!
                </p>
                <p><a href="https://www.harrysprojects.com/articles/maskrcnn.html"/>Next: Mask R-CNN</a></p>
                <br />
                <div id="disqus_thread"></div>
                <script>
                    var disqus_config = function () {
                    this.page.url = "https://www.harrysprojects.com/blogs/fasterrcnn.html";
                    this.page.identifier = "fasterrcnn";
                    };

                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://harrysprojects-com.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })();
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

                <br />
                <br />
            </div>

        </div>
    </body>
    <footer>
    </footer>
</html>