<!doctype html>

<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>SSD</title>
        <meta name="description" content="The SSD architecture and how it works.">
        <meta name="author" content="Harry Turner">
        <meta name="keywords" content="learn, tutorial, paper, machine learning, deep learning, rpn, region proposal network, object detection, SSD, loss function, cells, anchors, architecture, feature extraction, classification">
        <link rel="icon" type="image/ico" href="../images/favicon.ico">
        <link href="../styles/styles.css" type="text/css" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
        <link rel=“canonical” href=“https://www.harrysprojects.com/articles/rcnn.html” />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../js/Navbar.js" type="text/javascript" defer></script>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-180754451-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-180754451-1');
        </script>
    </head>
    <body>
        <div class="page-container">

            <nav-bar></nav-bar>

            <div>
                <h1>SSD</h1>
                <p><i>25th January 2021</i></p>
                <h3>Single Shot Detector</h3>
                <p>
                    In this post I introduce the Single Shot Detector (SSD), a detector that matched the accuracy of
                    two-stage architectures like Faster R-CNN, whilst remaining real-time like YOLO. SSD is fast because,
                    like YOLO, it is a one-stage architecture. However, one-stage architectures (at the time SSD was
                    released) suffered drawbacks that
                    limit their accuracy, so SSD implemented a few tricks to fix them, such as using multiple
                    feature maps and using separate predictors for different aspect ratios.
                </p>
                <h3>From Faster R-CNN to SSD</h3>
                <p>
                    SSD is not an evolution of Faster R-CNN, let me make that clear now. However, SSD and the Region
                    Proposal Network (RPN) used within Faster R-CNN are very similar. <i>Very very similar</i>. Since I've
                    already written extensively about Faster R-CNN and the RPN, I believe
                    a good introduction to SSD is to contrast and compare the two architectures. I believe that you will gain
                    more insight by first understanding the RPN, and then looking at how and why the SSD
                    architecture differs. I go into a lot of depth on Faster R-CNN in
                    <a href="https://www.harrysprojects.com/articles/fasterrcnn.html">this post</a>, and from now on I
                    shall assume that you are familiar with it.
                </p>
                <p>
                    With the above caveat in mind, I shall first introduce the architecture of SSD, and show how it
                    detects objects at different scales by doing detection on different feature maps of the convolutional
                    backbone. I'll then talk about a few other tricks the SSD paper implements to achieve good detection results,
                    including anchor design, and hard negative mining.
                </p>
                <h3>The Architecture</h3>
                <p>
                    First of all, the SSD paper refers to something called <i>default boxes.</i> Don't be fooled into
                    thinking this is a new concept, <i>they are identical to anchor boxes in every way.</i> Why they
                    didn't just call them anchors, I don't know.
                </p>
                <p>
                    Secondly, I want to quickly remind you of what a convolutional filter is. A <code>3x3</code> filter
                    that operates on a feature map with <code>p</code> channels has size <code>3x3xp</code> and produces
                    a single output for each position in the feature map. In practice,
                    we never just convolve one filter, we convolve several different filters to get several
                    different output channels. The number of outputs channels doesn't have to be equal to <code>p</code>.
                </p>
                <p>
                    Lastly, I will deliberately refer to something called a <i>convolutional layer</i> to mean a set of
                    such filters that all operate on an input feature map to generate an output feature map. If the input
                    feature map has <code>p</code> channels, and the output feature map has <code>q</code> channels, then
                    this convolutional layer consists of <code>q</code> different <code>3x3xp</code> filters that do not
                    share weights. <a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1">
                    This</a> is a really great resource that covers these concepts. I'm sure you're familiar with how
                    convolutional networks work so enough definitions, let's crack on.
                </p>
                <p>
                    It is instructive to first remind ourselves of the RPN. I have copied in Figure 1 from the Faster R-CNN
                    paper. Recall that the RPN is a convolutional feature extractor, with several convolutional layers
                    that take an image as input, and successively downsize it until you have a feature map with size <code>7x7x512</code>
                    or something similar (the size you end up with depends on the backbone you use). So far all we've described
                    is a network backbone, you can create a network backbone by taking an image classification architecture
                    and discarding the fully connected layers.
                </p>
                 <div class="image-container">
                    <img src="../images/articles/fasterrcnn/anchors_paper.png"/>
                    <p><i>Figure 1 from the Faster R-CNN paper, credit to Shaoqing Ren, Kaiming He, Ross Girshick and Jian Sun.</i></p>
                </div>
                <p>
                    The RPN predicts objects by sliding a convolutional filter over the feature map produced by the backbone.
                    This convolutional filter produces two branches, one with 2K scores, and the other with 4K coordinates,
                    and although the image above makes it look like these branches are fully connected, they are all in fact
                    computed in a convolutional manner using 1x1 convolutions. Recall that the 4K coordinates encode the
                    four coordinates of each box, whilst the 2K coordinates encode the class of each box, which is restricted
                    to just two classes: object or not object (remember the RPN does not predict a classification, it only
                    proposes boxes that might contain objects). Finally, recall that each position in the feature map is
                    associated with K different anchors, which each have different sizes and aspect ratios. This allows
                    a single feature map position to predict a variety of different shaped and sized boxes from the same
                    position. In the Faster R-CNN paper, K = 9.
                </p>
                <p>
                    Now I can introduce the SSD architecture by explaining it in terms of the differences between it
                    and the RPN. Firstly, the filter that is convolved with the feature map to
                    produce the detections <i>does not</i> consist of an intermediate layer followed by class and
                    regression layers. Instead, the whole thing is implemented with a single convolutional layer with (c+4)K outputs.
                    You can see the similarity with the RPN, the K refers to the number of anchors associated with the
                    feature map cell, the 4 refers to the four coordinates of the predicted box, but what's new is the C.
                    For SSD C is the number of classes, which we can now set to be whatever we like. If you make C = 2,
                    then you end up with the RPN. (2 + 4)K = 2K + 4K.
                </p>
                <p>
                    To put it a different way, a set of (C+4)K filters are each convolved with the entire
                    feature map to produce a vector of (C+4)K outputs for each feature map position, and these (C+4)K outputs
                    encode the four coordinates of a box and a distribution over classes for <i>K different boxes</i>. Those
                    K different boxes are of course the K different anchor sizes and aspect ratios for that cell.
                </p>
                <p>
                    One more thing to point out is that, just like the RPN, the background class is folded into the
                    class distribution, i.e it is one of the C classes. Just like the RPN has a 'no object' class and
                    'object' class that lets it do softmax over the distribution, SSD employs the same trick.
                </p>
                <h3>Anchor Design</h3>
                <p>
                    So far we've just discussed minor architecture differences, but SSD employs a new trick not seen
                    in any of the architectures we've looked at yet. They run their convolutional filters on multiple
                    feature maps. Not only does this produce more output detections, but the filters are using different
                    information to make their predictions, so we'll get a bigger variety of detected results. As well
                    as that, feature maps have implicit dimensions coded into them, for feature maps at the start with 14x14,
                    each cell will represent 1/14th of the width and heigth, whereas as 7x7 will end up looking for objects
                    twice as big. It also turns out that you don't have to keep the same definition of anchor aspects ratios
                    and sizes for each layer, you can different ones that tailor the system to achieve specific results.
                </p>
                <p>
                    The feature maps that they use to do the detection on are added onto the end. There's not much
                    point doing a convolution on two feature maps of the same size. So they add a few convolutions
                    to downsize them as much as possible. Although there's nothing special about these new layers, you
                    could run the filters over existing feature maps in the backbone just as effectively.
                </p>
                <h3>A Quick note on Training</h3>
                <p>
                    SSD parameterises the box regression using the same method as
                    <a href="https://www.harrysprojects.com/articles/fastrcnn.html">Fast R-CNN</a>. The loss function
                    also takes the same form as the loss function in Fast R-CNN, it uses the smooth L1 loss for
                    the regression loss, and the softmax/cross-entropy loss for the classes.
                </p>
                <h3>Hard Negative Mining</h3>
                <p>
                    Hard Negative Mining is a training stability trick that prevents the large number of negatives
                    from overwhelming the weight update during back propagation. Like most object detectors, SSD
                    predicts a large number of bounding boxes, over 8000, but there are certainly not 8000 objects in
                    the image. Even though the matching strategy in SSD can assign multiple predicted boxes to a single
                    ground truth, the vast majority of them will not be matched. What does the loss function do with
                    unmatched boxes? It ignores the localisation loss as it is undefined for an unmatched box, but
                    it maximises the liklihood of the background class. Because there are so many background boxes, it's
                    likely that our training signal will be overwhelmed by the background classes.
                </p>
                <p>
                    Hard Negative Mining is simple. It takes all the unmatched boxes and ranks them by the highest
                    confidence loss, and then picks the top ones so that the ratio between the negatives and positives
                    is at most <code>3:1</code>. This leads to faster optimisation and more stable training.
                </p>
                <br />
                <div id="disqus_thread"></div>
                <script>
                    var disqus_config = function () {
                    this.page.url = "https://www.harrysprojects.com/blogs/ssd.html";
                    this.page.identifier = "ssd";
                    };

                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://harrysprojects-com.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })();
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

                <br />
                <br />
            </div>

        </div>
    </body>
    <footer>
    </footer>
</html>