<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <!-- Review and updated March 2024
        canonical: yes
        navbar: yes
        images: yes
        disqus: no
        analytics: yes
        accessible: yes
        description: yes
        title: yes
        keywords: yes    
        -->
    <title>SSD</title>
    <meta name="description" content="The SSD architecture and how it works." />
    <meta name="author" content="Harry Turner" />
    <meta
      name="keywords"
      content="learn, tutorial, paper, machine learning, deep learning, rpn, region proposal network, object detection, SSD, loss function, cells, anchors, architecture, feature extraction, classification"
    />
    <link rel="icon" type="image/ico" href="/images/favicon.ico" />
    <link href="/styles/styles.css" type="text/css" rel="stylesheet" />
    <link
      href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap"
      rel="stylesheet"
    />
    <link rel=“canonical”
    href=“https://www.harrysprojects.com/pages/object_detection/ssd.html” />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script src="/js/Navbar.js" type="text/javascript" defer></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-X5NEKXRJE4"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-X5NEKXRJE4");
    </script>
  </head>
  <body>
    <div class="page-container">
      <nav-bar></nav-bar>

      <div>
        <h1>SSD</h1>
        <p><i>25th January 2021</i></p>
        <h3>Single Shot Detector</h3>
        <p>
          In this post I introduce the Single Shot Detector (SSD), a detector
          that matched the accuracy of two-stage architectures like Faster
          R-CNN, whilst remaining real-time like YOLO. SSD is fast because, like
          YOLO, it is a one-stage architecture. However, one-stage architectures
          (at the time SSD was released) suffered drawbacks that limit their
          accuracy, so SSD implemented a few tricks to fix them, such as using
          multiple feature maps and using separate predictors for different
          aspect ratios.
        </p>
        <h3>From Faster R-CNN to SSD</h3>
        <p>
          SSD is not an evolution of Faster R-CNN, let me make that clear now.
          However, SSD and the Region Proposal Network (RPN) used within Faster
          R-CNN are very similar. <i>Very very similar</i>. Since I've already
          written extensively about Faster R-CNN and the RPN, I believe a good
          introduction to SSD is to contrast and compare the two architectures.
          I believe that you will gain more insight by first understanding the
          RPN, and then looking at how and why the SSD architecture differs. I
          go into a lot of depth on Faster R-CNN in
          <a
            href="https://www.harrysprojects.com/pages/object_detection/fasterrcnn.html"
            >this post</a
          >, and from now on I shall assume that you are familiar with it.
        </p>
        <p>
          With the above caveat in mind, I shall first introduce the
          architecture of SSD, and show how it detects objects at different
          scales by doing detection on different feature maps of the
          convolutional backbone. I'll then talk about a few other tricks the
          SSD paper implements to achieve good detection results, including
          anchor design, and hard negative mining.
        </p>
        <h3>The Architecture</h3>
        <p>
          First of all, the SSD paper refers to something called
          <i>default boxes.</i> Don't be fooled into thinking this is a new
          concept, <i>they are identical to anchor boxes in every way.</i> Why
          they didn't just call them anchors, I don't know.
        </p>
        <p>
          Secondly, I want to quickly remind you of what a convolutional filter
          is. A <code>3x3</code> filter that operates on a feature map with
          <code>p</code> channels has size <code>3x3xp</code> and produces a
          single output for each position in the feature map. In practice, we
          never just convolve one filter, we convolve several different filters
          to get several different output channels. The number of outputs
          channels doesn't have to be equal to <code>p</code>.
        </p>
        <p>
          Lastly, I will deliberately refer to something called a
          <i>convolutional layer</i> to mean a set of such filters that all
          operate on an input feature map to generate an output feature map. If
          the input feature map has <code>p</code> channels, and the output
          feature map has <code>q</code> channels, then this convolutional layer
          consists of <code>q</code> different <code>3x3xp</code> filters that
          do not share weights.
          <a
            href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1"
          >
            This</a
          >
          is a really great resource that covers these concepts. I'm sure you're
          familiar with how convolutional networks work so enough definitions,
          let's crack on.
        </p>
        <p>
          It is instructive to first remind ourselves of the RPN. I have copied
          in Figure 1 from the Faster R-CNN paper. Recall that the RPN is a
          convolutional feature extractor, with several convolutional layers
          that take an image as input, and successively downsize it until you
          have a feature map with size <code>7x7x512</code>
          or something similar (the size you end up with depends on the backbone
          you use). So far all we've described is a network backbone, you can
          create a network backbone by taking an image classification
          architecture and discarding the fully connected layers.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/anchors_paper.png" />
          <p>
            <i
              >Figure 1 from the Faster R-CNN paper, credit to Shaoqing Ren,
              Kaiming He, Ross Girshick and Jian Sun.</i
            >
          </p>
        </div>
        <p>
          The RPN predicts objects by sliding a convolutional filter over the
          feature map produced by the backbone. This convolutional filter
          produces two branches, one with 2K scores, and the other with 4K
          coordinates, and although the image above makes it look like these
          branches are fully connected, they are all in fact computed in a
          convolutional manner using 1x1 convolutions. Recall that the 4K
          coordinates encode the four coordinates of each box, whilst the 2K
          coordinates encode the class of each box, which is restricted to just
          two classes: object or not object (remember the RPN does not predict a
          classification, it only proposes boxes that might contain objects).
          Finally, recall that each position in the feature map is associated
          with K different anchors, which each have different sizes and aspect
          ratios. This allows a single feature map position to predict a variety
          of different shaped and sized boxes from the same position. In the
          Faster R-CNN paper, K = 9.
        </p>
        <p>
          Now I can introduce the SSD architecture by explaining it in terms of
          the differences between it and the RPN. Firstly, the filter that is
          convolved with the feature map to produce the detections
          <i>does not</i> consist of an intermediate layer followed by class and
          regression layers. Instead, the whole thing is implemented with a
          single convolutional layer with (c+4)K outputs. You can see the
          similarity with the RPN, the K refers to the number of anchors
          associated with the feature map cell, the 4 refers to the four
          coordinates of the predicted box, but what's new is the C. For SSD C
          is the number of classes, which we can now set to be whatever we like.
          If you make C = 2, then you end up with the RPN. (2 + 4)K = 2K + 4K.
        </p>
        <p>
          To put it a different way, a set of (C+4)K filters are each convolved
          with the entire feature map to produce a vector of (C+4)K outputs for
          each feature map position, and these (C+4)K outputs encode the four
          coordinates of a box and a distribution over classes for
          <i>K different boxes</i>. Those K different boxes are of course the K
          different anchor sizes and aspect ratios for that cell.
        </p>
        <p>
          One more thing to point out is that, just like the RPN, the background
          class is folded into the class distribution, i.e it is one of the C
          classes. Just like the RPN has a 'no object' class and 'object' class
          that lets it do softmax over the distribution, SSD employs the same
          trick.
        </p>
        <h3>Anchor Design</h3>
        <p>
          So far we've just discussed minor architecture differences, but SSD
          employs a new trick not seen in any of the architectures we've looked
          at yet. They run their convolutional filters on multiple feature maps.
          Not only does this produce more output detections, but the filters are
          using different information to make their predictions, so we'll get a
          bigger variety of detected results. As well as that, feature maps have
          implicit dimensions coded into them, for feature maps at the start
          with 14x14, each cell will represent 1/14th of the width and heigth,
          whereas as 7x7 will end up looking for objects twice as big. It also
          turns out that you don't have to keep the same definition of anchor
          aspects ratios and sizes for each layer, you can different ones that
          tailor the system to achieve specific results.
        </p>
        <p>
          The feature maps that they use to do the detection on are added onto
          the end. There's not much point doing a convolution on two feature
          maps of the same size. So they add a few convolutions to downsize them
          as much as possible. Although there's nothing special about these new
          layers, you could run the filters over existing feature maps in the
          backbone just as effectively.
        </p>
        <h3>A Quick note on Training</h3>
        <p>
          SSD parameterises the box regression using the same method as
          <a
            href="https://www.harrysprojects.com/pages/object_detection/fastrcnn.html"
            >Fast R-CNN</a
          >. The loss function also takes the same form as the loss function in
          Fast R-CNN, it uses the smooth L1 loss for the regression loss, and
          the softmax/cross-entropy loss for the classes.
        </p>
        <h3>Hard Negative Mining</h3>
        <p>
          Hard Negative Mining is a training stability trick that prevents the
          large number of negatives from overwhelming the weight update during
          back propagation. Like most object detectors, SSD predicts a large
          number of bounding boxes, over 8000, but there are certainly not 8000
          objects in the image. Even though the matching strategy in SSD can
          assign multiple predicted boxes to a single ground truth, the vast
          majority of them will not be matched. What does the loss function do
          with unmatched boxes? It ignores the localisation loss as it is
          undefined for an unmatched box, but it maximises the liklihood of the
          background class. Because there are so many background boxes, it's
          likely that our training signal will be overwhelmed by the background
          classes.
        </p>
        <p>
          Hard Negative Mining is simple. It takes all the unmatched boxes and
          ranks them by the highest confidence loss, and then picks the top ones
          so that the ratio between the negatives and positives is at most
          <code>3:1</code>. This leads to faster optimisation and more stable
          training.
        </p>
        <br />
        <div id="disqus_thread"></div>
        <script>
          var disqus_config = function () {
            this.page.url =
              "https://www.harrysprojects.com/pages/object_detection/ssd.html";
            this.page.identifier = "ssd";
          };

          (function () {
            // DON'T EDIT BELOW THIS LINE
            var d = document,
              s = d.createElement("script");
            s.src = "https://harrysprojects-com.disqus.com/embed.js";
            s.setAttribute("data-timestamp", +new Date());
            (d.head || d.body).appendChild(s);
          })();
        </script>
        <noscript
          >Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript"
            >comments powered by Disqus.</a
          ></noscript
        >

        <br />
        <br />
      </div>
    </div>
  </body>
  <footer></footer>
</html>
