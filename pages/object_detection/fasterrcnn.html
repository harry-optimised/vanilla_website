<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <!-- Review and updated March 2024
        canonical: yes
        navbar: yes
        images: yes
        disqus: no
        analytics: yes
        accessible: yes
        description: yes
        title: yes
        keywords: yes    
        -->
    <title>Faster R-CNN</title>
    <meta
      name="description"
      content="How the faster r-cnn deep object detector works."
    />
    <meta name="author" content="Harry Turner" />
    <meta
      name="keywords"
      content="learn, tutorial, paper, deep learning, object detection, faster r-cnn, architecture, region proposal network, anchors, dense boxes, training, machine learning"
    />
    <link rel="icon" type="image/ico" href="/images/favicon.ico" />
    <link href="/styles/styles.css" type="text/css" rel="stylesheet" />
    <link
      href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap"
      rel="stylesheet"
    />
    <link rel=“canonical”
    href=“https://www.harrysprojects.com/pages/object_detection/fasterrcnn.html”
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="/js/Navbar.js" type="text/javascript" defer></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-180754451-1"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "UA-180754451-1");
    </script>
  </head>
  <body>
    <div class="page-container">
      <nav-bar></nav-bar>

      <div>
        <h1>Faster R-CNN.</h1>
        <p><i>11th October 2020</i></p>
        <h3>Faster R-CNN</h3>
        <p>
          Faster R-CNN is a powerful, modern two-stage object detector still in
          use today. As well as that, it provides a framework for understanding
          some of the innovations coming from Facebook AI Research, namely Mask
          R-CNN, Panoptic Segmentation, and Mesh R-CNN. It's well worth spending
          some time to understand this model.
        </p>
        <p>
          If you've not done so already, I suggest you read the previous post on
          <a
            href="https://www.harrysprojects.com/pages/object_detection/fasterrcnn.html"
            >Fast R-CNN</a
          >, as this post builds on that one and generalises all those concepts
          to the family of two stage object detectors. Faster R-CNN is about as
          complex as it gets for now, later papers add things here and there but
          the details are in the additions, if you can understand the framework
          that Faster R-CNN provides, it'll really help to organise your
          thinking. I spend the first part of this post presenting the
          architectures of a few modern object detectors in terms of a framework
          of building blocks, it might get a bit repetitive but it's an
          important point so please bear with me.
        </p>
        <p>
          Also note that I flip back and forth between discussing Fast R-CNN and
          <i>Faster R-CNN</i>, I'm being very specific in my terminology, it's a
          shame the names are so similar.
        </p>
        <h3>How to Think About It</h3>
        <p>
          Let's start by analysing Fast R-CNN in terms of building blocks. We
          have a block for region proposal, which is Selective Search, a block
          for extracting features of those proposals, (a CNN backbone), a block
          for classifying proposals, and a block for finetuning the boxes of
          those proposals, both of which are branches of a detector head.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/blocks_fastrcnn.png" />
          <p><i>The building blocks of Fast R-CNN.</i></p>
        </div>
        <p>
          Now we can swap in and out different blocks, we could use SIFT
          features instead of a CNN, or SVMs instead of the classification
          branch. But neither of those improve on Fast R-CNN. We could use the
          new (as of 2014) proposal mechanism called Edge Boxes instead of
          Selective Search, it doesn't produce better boxes but it's faster. Or
          we could use dense boxes instead. What are dense boxes? I'll cover
          them in more detail when we talk about anchors, but essentially you
          propose a box at <i>every</i> position in the image, at a range of
          scales and aspect ratios. Contrast this to Selective Search which
          tries to only propose boxes that might contain something.
        </p>
        <p>
          The point I'm making in this section is that you can think of a large
          number of object detectors as being made up of these components.
        </p>
        <p>
          Fast R-CNN is Selective Search + backbone + two small networks for
          regression and classification, cleverly tied together to enable end to
          end training. If you use dense boxes instead, you end up with a one
          stage network that is <i>similar</i> to a detector called OverFeat.
          (The reason it is suddenly called one-stage, is because there is
          practically no computation for dense boxes, they are predefined). The
          performance difference will be down to whether dense boxes recall more
          objects than Selective Search. It becomes really useful to think about
          these detectors in this way.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/blocks_overfeat.png" />
          <p><i>The building blocks of R-CNN and OverFeat.</i></p>
        </div>
        <p>
          Now.. what is a region proposal mechanism really..? It's something
          that proposes regions that hopefully contain objects. So it's
          something that.. detects objects? Isn't our region proposal mechanism
          at heart just another object detector?? Yes, and so we should be able
          to do it with a network!
        </p>
        <p>
          Let's swap the Selective Search building block out for a neural
          network based proposal mechanism, let's call it a
          <i>Region Proposal Network</i>.
        </p>
        <div class="image-container">
          <img
            src="/images/object_detection/fasterrcnn/blocks_fasterrcnn.png"
          />
          <p><i>The building blocks of Faster R-CNN.</i></p>
        </div>
        <p>
          It turns out the Faster R-CNN it's basically two objects detectors
          back to back, in a cascade, welcome to the king of two-stage
          detectors. As I said a few lines ago, it's useful to think about it as
          <i> replacing Selective Search</i> with a neural network. Not only
          does this turn out to be faster, but it proposes much better regions
          as we'll see. The rest of the network is just normal Fast R-CNN, so
          the same backbone, the same classifier head and same regressor head.
          Thinking about networks in this way is simple but powerful, I hope you
          see what I'm getting at.
        </p>
        <h3>The Architecture</h3>
        <p>
          I have a confession to make, the diagram of Faster R-CNN above is a
          lie, kind of. Although there are two networks, they're not separated
          quite so cleanly, I said it's useful to
          <i> think about the network like that</i> but it is implemented in a
          more efficient way. Specifically, we are going to end up with two
          object detectors, but they're going to share certain building blocks
          for efficiency. Therefore it will be useful to define some specific
          terminology to allow us to be precise. This will be largely consistent
          with the literature, although the literature is not exactly consistent
          either.
        </p>
        <p>
          We have two networks, the first will propose regions, and it's called
          the <i>Region Proposal Network</i>, the second will detect objects,
          we'll call it the <i>Detector Network</i>. In a previous post, I said
          you could think about object detectors as having backbones, and heads.
          So we have a <i>Region Proposal Network Backbone</i>, a
          <i>Region Proposal Network Head</i>, a <i>Detector Backbone</i>, and a
          <i>Detector Head</i>. The job of the Region Proposal Network Head is
          to predict region proposals that contain <i>any</i> object, the job of
          the Detector Head is to classify those proposals and finetune them
          into actual object bounding box predictions.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/terminology.png" />
          <p>
            <i
              >Faster R-CNN is made up of a Region Proposal Network and a
              Detector Network.</i
            >
          </p>
        </div>
        <p>
          Now we come to our first implementation detail,
          <i>both networks share the same backbone. </i> We can do this because
          both networks are doing object detection on the same image, and
          features useful for predicting proposals are probably also useful for
          classifying and regressing those proposals. From now on, I'll just
          call it <i>the backbone</i>. So currently our architecture looks like
          this.
        </p>
        <div class="image-container">
          <img
            src="/images/object_detection/fasterrcnn/combined_backbone.png"
          />
          <p>
            <i
              >The Region Proposal Network and Detector share the same
              backbone.</i
            >
          </p>
        </div>
        <p>
          Remember how the Fast R-CNN head received region proposals and feature
          maps, and it first used an ROI Pooling layer to warp all projected
          feature regions into a fixed length vector before doing classification
          and regression. Well all those pieces still exist here, so
          <i
            >you already know how the Detector Head works, it's just the Fast
            R-CNN head.</i
          >
        </p>
        <p>
          Likewise, the backbone is exactly the same as in Fast R-CNN as well,
          it produces features that are consumed by both the Region Proposal
          Head and the Detector Head. The new part is the Region Proposal Head,
          which we'll look at in a minute.
        </p>
        <p>
          There's still a missing piece, if the Region Proposal Network (or
          <i>RPN</i> as I'll start to call it) is proposing regions for the
          Detector Network, then
          <i>what is proposing regions for the Region Proposal Network?</i> You
          read that right, the RPN is an object detector now, and all the object
          detectors we've looked at so far used modules such as Selective
          Search, it can't just magic proposals from thin air.
          <i>For now, let's just give it dense proposals.</i> It's not quite as
          simple as that, we'll come back to it shortly and develop the idea
          into anchors which is how it actually works.
        </p>
        <div class="image-container">
          <img
            src="/images/object_detection/fasterrcnn/with_dense_proposals.png"
          />
          <p>
            <i
              >The Region Proposal Network get's it's proposals from something
              called Anchors, which are a (bit) like dense proposals.</i
            >
          </p>
        </div>
        <p>
          And so finally we're ready to explore the details. The
          <i>architecture</i> presented directly above is missing a few details
          but it's enough for the following discussions. The RPN starts with a
          <i> dense box-like process</i> and predicts region proposals, using a
          CNN backbone to extract the features from those dense boxes. The
          Detector also uses those features, but uses the region proposals from
          the RPN instead. From that point, it's just Fast R-CNN doing it's
          normal thing of classifying into classes and finetuning.
        </p>
        <p>
          From this perspective, you can see that the RPN is just a clever way
          of turning naive dense boxes into much better region proposals, that
          outperforms Selective Search. That's <i>basically</i> all that Faster
          R-CNN is, plus a few tricks which we'll see shortly.
        </p>
        <p>
          Coming up, we'll address those dense boxes and develop the idea of
          <i>Anchors</i> which is how it actually works, we'll look at the RPN
          Head and how it's just a variant of the Detector Head, and we'll
          discuss a little bit about how all the pieces are wired together.
        </p>
        <h3>Dense Boxes</h3>
        <p>
          The RPN Head needs to get it's box proposals from <i>somewhere</i>,
          it's still an object detector. I said previously that we'd supply it
          dense boxes, let's look a bit more closely at what those are.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/anchors.png" />
          <p><i>Dense Box Proposals</i></p>
        </div>
        <p>
          Look at the image above, our goal is to detect the dog. Selective
          Search would propose regions that likely contained objects, hopefully
          one of them would be the dog, and our network head would classify and
          finetune it. So with dense boxes, we need to be able to propose boxes
          that at least get close to containing the dog. But the dog could also
          be at any position, so we need to be invariant to location as well. We
          can do this by sampling a regular grid of boxes over the entire image.
        </p>
        <p>
          Start with the top left image, I divided it into squares, each square
          is a region proposal. If you count them all there are 35 region
          proposals, that's nothing compared to the 2000 we get from Selective
          Search. We need more boxes, and we need them to be more finegrained,
          so a natural next step would be to let them overlap and sample more
          frequently. This is shown in the top right image, although it's
          getting a bit messy now. Let's say we doubled our sampling frequency
          in each direction so we've now got about 3 times the number of boxes,
          we're up to 100. (Don't bother counting them all).
        </p>
        <p>
          Each region proposal so far is still quite small, none of the are
          really capable of detecting the dog. Even the one right in the middle
          of the dogs face would have to be regressed several times larger than
          itself to fit. Let's add more proposals. For
          <i>every proposal we already have</i>, we're going to add two more at
          the same position, one at about double scale and one about triple
          scale. In the bottom left image I've shown this for one square in red
          because it starts getting very messy but if you imagine we do that for
          every box proposal, we're up to 300 proposals.
        </p>
        <p>
          You can probably see where this is going now, the last step is to add
          a few different aspect ratios, so, in
          <i> addition</i> to the different sizes we already have, for
          <i>each</i> original white box proposal we have nine now, three sizes
          at three different aspect ratios. I have shown height bigger than
          width to illustrate, but the same applies to width bigger than height.
          We're at 900 region proposals.
        </p>
        <p>
          These dense boxes are incredibly quick to compute, notice how we've
          predefined them, it doesn't matter what the image is, we always sample
          the same way. This can be done with a few lines of numpy code, no slow
          Selective Search or Edge Boxes. We call them dense because they
          densely cover the entire image, simple.
        </p>
        <h3>Anchors</h3>
        <p>
          If you understand dense boxes, you can understand anchors too. The key
          piece of intuition is that we don't sample a regular grid of boxes on
          the <i>image</i> and send those to the RPN Head, <i>instead</i>, we
          sample a regular grid of boxes
          <i>from the last convolutional map.</i> The best illustration of this
          is from the paper itself, let me copy it in below.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/anchors_paper.png" />
          <p>
            <i
              >Figure 1 from the Faster R-CNN paper, credit to Shaoqing Ren,
              Kaiming He, Ross Girshick and Jian Sun.</i
            >
          </p>
        </div>
        <p>
          Look at the convolutional map, doesn't it look just like a regular
          grid of boxes? We don't even need to sample, we can just take
          <i>every position</i> in the feature map as a dense box proposal. A
          typical backbone might give us a feature map of size 15x15 (depending
          on the backbone), so that's 225 proposals right there. If we do the
          scale and aspect ratio trick, we get about 2000 proposals.
        </p>
        <p>
          Each position in the feature map has an
          <a
            href="https://blog.christianperone.com/2017/11/the-effective-receptive-field-on-cnns/"
            >effective receptive field
          </a>
          that covers quite a large patch of the original image, so it's
          <i
            >as if we had a whole load of overlapping box proposals on the
            original image</i
          >, except we can do it all on the feature map instead.
        </p>
        <p>
          So just to recap where we are, we've got a feature map of size 15x15,
          each of the 225 positions in that feature map are our box proposals,
          and for each one we use 3 different scales and 3 different aspect
          ratios to get about 2000 box proposals.
          <i
            >These special box proposals coming from the feature map are called
            anchors.</i
          >
        </p>
        <p>
          In Fast R-CNN, we discussed how the network head turns region
          proposals into fixed length vectors by passing them through a special
          layer called the Region Pooling Layer.
          <i>We don't need to do this for anchors</i>. Because they are defined
          on the feature map, we can do something clever with convolutions. I'll
          come back to this in a minute when I talk about the RPN Head, but for
          now, know that we can go straight from feature map to predicted boxes
          with no Region Pooling step.
        </p>
        <p>
          The last piece of the puzzle is to understand how anchor boxes are
          parameterised at runtime. In other words, how do you go from anchor
          box to predicted box on the final image? Recall that Fast R-CNN
          outputs four numbers for each box, but these four numbers are
          parameterised scales and offsets for each box proposal, I discussed
          this in detail at the end of the Fast R-CNN post. To be even more
          explicit, the box proposal from Selective Search is in actual pixel
          coordinates, but is shifted and scaled by the four outputs from the
          network in a special way that makes sure the box doesn't get turned
          inside out. See the end of the Fast R-CNN post if that didn't make
          sense.
        </p>
        <p>
          Firstly, we must figure out what pixel coordinates our anchors
          correspond to in the actual image. Fortunately we can do this quite
          easily by mapping the size of the feature map onto the size of the
          image. For example, in our 15x15 feature map example, if the image was
          150x150 pixels then each anchor is <i>positioned </i> every 10 pixels
          in the actual image. How big is each anchor in the original image?
          Well we get to choose. Because we have 3 different scales for our
          boxes, we might define one at 10x10, one at 30x30, and one at 50x50,
          but it's entirely up to you. The important thing is that for
          <i>every anchor defined on your feature map</i>, there is an
          equivalent set of corresponding
          <i>pixel coordinates in the original image.</i>
        </p>
        <p>
          Secondly, once we have the four outputs from the network, we can use
          the parameterisation equations from the R-CNN paper to
          <i>convert</i> the anchor pixel coordinates (which we know) into our
          predicted box pixel coordinates. I have repeated these equations
          below, the <code>dx, dy, dw, dh </code> are the four outputs from the
          network, <code>Ax, Ay, Aw, Ah </code>
          are the anchor coordinates in pixels.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/anchor_targets.gif" />
          <p>
            <i
              >Equations for computing box coordinates from anchors and network
              outputs.</i
            >
          </p>
        </div>
        <p>
          That's how the anchors work at runtime, there is a section in the
          paper called <i>A Loss Function for Learning Region Proposals </i>
          that describes how this process works during training as well. I made
          a conscious decision not to cover it here because this post is already
          getting long, however if you've been following along then you have all
          the bits you need to understand it. The trick is that for training you
          also have to parameterise the anchors with respect to a ground truth
          box, (think of this as: what scaling and offset transformation would I
          have to apply to this anchor in order to warp it into the ground truth
          box). Those four parameterised numbers are what you train the network
          to output.
        </p>
        <h3>The RPN Head</h3>
        <p>
          The RPN Head turns anchors into actual box proposals in pixel
          coordinates, because that's what the Fast R-CNN detector head expects.
          But as I said earlier, you don't need to warp anchors using a Region
          Pooling layer, you can do it all with a clever convolution.
        </p>
        <p>
          If you read the section in the paper that describes anchors, you'll
          find that they initially define it as sliding a small network over the
          feature map, they then state that this is naturally implemented by a
          convolutional layer. This is what they mean, first take a look at this
          image again.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/fasterrcnn/anchors_paper.png" />
          <p>
            <i
              >Figure 1 from the Faster R-CNN paper, credit to Shaoqing Ren,
              Kaiming He, Ross Girshick and Jian Sun.</i
            >
          </p>
        </div>
        <p>
          We're already starting at a feature map, I'm going to call it the
          semantic feature map because although the diagram shows just one
          channel for easier visualisation, it actually has lots of channels,
          lets say 512, each hopefully encoding for specific meaningful
          features. If we do a convolution on this feature map, what do we get?
          Well, another feature map, but lets say our convolution downsizes from
          512 channels to just 4 in our output feature map. Is it possible we
          could interpret each of those four channels as the four regression
          outputs of the network? Well yes definitely, the maths will work, and
          as long as we train the outputs to match the parameterised targets we
          came up with earlier, this will actually work. This is sort of similar
          to what Fast R-CNN is doing
          <i>except we're doing it with convolutions instead.</i>
        </p>
        <p>
          But we don't just need regression outputs, we need class scores as
          well. For the RPN Head, we actually have just two class scores,
          object, and not object, but we still need a layer for this. Here's
          what actually happens. We start with our semantic feature map of 512
          channels, we first do a convolution to downsize from 512 to
          <i>256</i> channels, let's call this the
          <i>intermediate layer</i> (note that the height and width of the
          feature map remain the same throughout this whole process). Then, we
          do a <i> 1x1 convolution</i> on the intermediate layer to downsize it
          from 256 to 4, this produces <i>another feature map </i> that we
          interpret as our regression outputs. Then, we do another convolution
          <i>on the same intermediate layer</i>, which produces a fourth feature
          map with just 2 channels, the classes. So both the regression and
          class feature maps are produced <i>from the intermediate layer</i>.
          This is how we branch in the world of convolutions, we do it with
          1x1s. It's unfortuneate that the diagram above makes it look like
          fully connected layers.
        </p>
        <p>
          Hopefully that diagram now makes sense to you. Each position in the
          feature map has K anchors associated with it, K being 9 in this case,
          for all those scales and aspect ratios. They do a convolution to
          produce the intermediate feature map which has 256 channels, then they
          do two 1x1 convolutions to produce the output branches, with 2K scores
          and 4K coordinates respectively. Why 2K? 2 classes for every K box
          <i>for that specific position.</i> 2K scores and 4K coordinates are
          predicted <i>for every single position in the feature map.</i>
        </p>
        <p>
          That's the Region Proposal Head, a convolution layer followed by two
          sibling 1x1 convolution layers.
        </p>
        <p>
          You're probably getting fed up of all this repeition, but I personally
          struggled with anchors. It was a long time before they finally clicked
          and this part is what tripped me up.
        </p>
        <h3>A Few Bits and Pieces</h3>
        <p>
          Because of all the anchors, the RPN outputs <i>a lot</i> of boxes. But
          not all of them are useful. Therefore there is a Non Maximum
          Suppression (NMS) step <i>in between</i> the output of the RPN and the
          input to the Detector Head. The threshold is set to 0.7, which ends up
          filtering the number of boxes down to about 2000 (same as Selective
          Search), by keeping the regions with the highest objectness score.
          This doesn't harm the accuracy of the detector, it just reduces the
          number of proposals it has to process.
        </p>
        <p>
          Why bother with two stage at all, it sounds like the RPN could be
          adapated to just do the whole thing by giving it more than two classes
          to predict. Well remember at the beginning when I talked about
          building blocks that this is basically what OverFeat is, and indeed
          this is the bridge to one stage detectors. The authors defend their
          decision for two stages by describing it as a sort of attention
          mechnism, the detector attends to the proposals. There is an advantage
          to this in that it quickly removes a lot of false positives, something
          that one stage detectors like YOLO struggle with. It is however,
          comparatively slower.
        </p>
        <h3>Training</h3>
        <p>
          I said above that I'm not going to cover the training of Faster R-CNN,
          because it's already quite a heavy going post, and you already have
          the bits you need to understand it anyway. We touched on training of
          Fast R-CNN in the last post and the RPN is trained in a similar way.
        </p>
        <p>
          With that said however, I do want to point out one detail, and that's
          that we're back into the regime of multi-stage training. Specifically,
          we first train the RPN, then we train a separate Fast R-CNN with those
          proposals, then we use those weights to initialise the RPN (again),
          then finetune the RPN and then finetune the Fast R-CNN head (again).
        </p>
        <p>
          If that didn't make sense, don't worry, it wasn't meant to. It's all a
          bit messy and it's a hard thing to describe in words, in fact the
          paper does a good job of explaining it so have a look there if you are
          interested. No, all I wanted to point out is that we're back to
          multistage training again because of the complexity of having another
          network you need to train.
        </p>
        <p>
          What I think <i>is</i> interesting is to consider why you need this
          multistage regime, what would happen if you just trained it end to end
          (which is the trend nowadays). They speculate on the reason in the
          paper and then leave it to future work, but the gist is that your Fast
          R-CNN head relies on (good) proposals from the region proposal
          mechanism, otherwise it cannot be trained at all, but early in
          training the RPN is probably not generating very good proposals. The
          distribution of proposals going into the Fast R-CNN head
          <i>is constantly moving</i>, which I assume makes it very hard for the
          Fast R-CNN head to learn anything as it can't rely on it's inputs!
        </p>
        <p>
          I expect this makes the training unstable, and you'd have to wait for
          the RPN to be trained to the point where it's generating half decent
          proposals before the Fast R-CNN head began to improve. Anyway, that's
          all I wanted to mention about that, mostly I thought it was
          insightful.
        </p>
        <h3>And We're Done</h3>
        <p>
          That's it for Faster R-CNN. What I hoped to do in this post was build
          up a bit of a framework for thinking about object detection
          architectures. Faster R-CNN is still in active use today, modern
          versions of it are slightly augmented versions that improve on it in
          various ways, we'll see those improvements soon. We'll also see the
          evolution of Faster R-CNN into other vision tasks like mask prediction
          and even 2D to 3D prediction. The good news is, if you followed these
          object detection posts so far, you'll find these next few papers
          natural extensions of what we've already discussed and should be able
          to fit them in quite nicely into what you already know and understand.
        </p>
        <p>Onwards!</p>
        <p>
          <a
            href="https://www.harrysprojects.com/pages/object_detection/maskrcnn.html"
            >Next: Mask R-CNN</a
          >
        </p>
        <br />
        <div id="disqus_thread"></div>
        <script>
          var disqus_config = function () {
            this.page.url =
              "https://www.harrysprojects.com/pages/object_detection/fasterrcnn.html";
            this.page.identifier = "fasterrcnn";
          };

          (function () {
            // DON'T EDIT BELOW THIS LINE
            var d = document,
              s = d.createElement("script");
            s.src = "https://harrysprojects-com.disqus.com/embed.js";
            s.setAttribute("data-timestamp", +new Date());
            (d.head || d.body).appendChild(s);
          })();
        </script>
        <noscript
          >Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript"
            >comments powered by Disqus.</a
          ></noscript
        >

        <br />
        <br />
      </div>
    </div>
  </body>
  <footer></footer>
</html>
