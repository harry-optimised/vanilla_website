<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <!-- Review and updated March 2024
        canonical: yes
        navbar: yes
        images: yes
        disqus: no
        analytics: yes
        accessible: yes
        description: yes
        title: yes
        keywords: yes    
        -->
    <title>Spatial Pyramid Pooling</title>
    <meta name="description" content="The SPP architecture and how it works." />
    <meta name="author" content="Harry Turner" />
    <meta
      name="keywords"
      content="learn, tutorial, paper, spatial pyramid pooling, deep learning, machine learning, object detection, spp layer"
    />
    <link rel="icon" type="image/ico" href="/images/favicon.ico" />
    <link href="/styles/styles.css" type="text/css" rel="stylesheet" />
    <link
      href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap"
      rel="stylesheet"
    />
    <link rel=“canonical”
    href=“https://www.harrysprojects.com/pages/object_detection/spp.html” />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="/js/Navbar.js" type="text/javascript" defer></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-180754451-1"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "UA-180754451-1");
    </script>
  </head>
  <body>
    <div class="page-container">
      <nav-bar></nav-bar>

      <div>
        <h1>Spatial Pyramid Pooling.</h1>
        <p><i>20th September 2020</i></p>
        <h3>Spatial Pyramid Pooling</h3>
        <p>
          Shortly after R-CNN was released, came
          <a href="https://arxiv.org/abs/1406.4729">SPP-Net</a>, which
          demonstrated what I refer to as a generally applicable result,
          something that could be applied to many different networks in object
          detection, classification, segmentation, etc, and provide significant
          improvements in all. These papers are usually well worth reading.
        </p>
        <p>
          Their insight was that up to this point, all these tasks used
          convolutional networks with fully connected layers at the end, and
          that these fully connected layers enforced a fixed size constraint on
          the input image. In other words, if the first fully connected layer
          went from <code>A</code> inputs to <code>B</code> outputs, then the
          <i>flattened and resized</i> output from the last convolutional layer
          <i>must</i> be of size <code>A</code> (otherwise the matrix
          computation wouldn't line up), which fixes the next convolutional
          layer, and the next, all the way to the input. This is why AlexNet
          needs an input of <code>227 x 227</code>. SPP-Net introduced a new
          layer between the final convolutional layer and the fully connected
          layer that removes this constraint.
        </p>
        <p>
          It's called a spatial pyramid pooling layer and here's how it works.
          Refer to the image below, I'll explain it from the bottom of the image
          up. Once you've passed your image through the convolutional layers,
          you get a set of feature maps, for example, let's say we get 256
          feature maps. You max pool each feature map to produce a vector of
          length 256 (remember max pool picks the largest value in each feature
          map and there are 256 maps). So you've now got a vector of 256.
          Technically, this vector represents a summarised version of our
          feature maps, you could pass that vector on to the final
          classification layer. In other words, the SPP layer has received a set
          of convolutional feature maps as input and flattened them into a 256
          length vector. But we can do better.
        </p>
        <p>
          We'll do the same thing again, but this time you split each feature
          map into four quadrants (bins) and do a max pool in each bin, this
          gives you four numbers for each of the 256 feature maps, or four lots
          of 256-length vectors, then you do it again but split each feature map
          into 16 bins which gives you 16 lots of 256-length vectors. Now lets
          stop there and stack all those vectors up, your final vector is made
          up of stacking all those 256-length vectors, the 16 from the lowest
          level plus the 4 from the next level plus the global one to produce 21
          lots of 256-length vectors, or a single vector of length 5376. It is
          this final vector that we end up passing on to the later layers in the
          network.
        </p>
        <p>
          Make sure you understand the significance, work this through in your
          mind: <i>no matter what size the input is</i>, it will always produce
          a vector of length 5376 (for that particular configuration of bins).
        </p>
        <div class="image-container">
          <img src="/images/object_detection/spp/spp-working.png" />
          <p>
            <i
              >This is Figure 3 from the SPP-Net paper, credit goes to the paper
              authors.</i
            >
          </p>
        </div>
        <p>
          Why is this relevant to object detection? Most of the SPP-Net paper is
          about the authors applying their new idea to lots of different domains
          and showing how it improves performance in all of them, including
          R-CNN! They make a very important observation, they recognise that the
          speed bottle neck in R-CNN is because the CNN is applied to every
          <i>image</i> region (all 2000), meaning it is run 2000 times. But they
          realised that they could run the CNN feature extractor <i>once</i> on
          the entire image to produce a set of feature maps, and
          <i>then</i> apply their SPP-Pooling layer to each region.
        </p>
        <p>
          Let me repeat that again, R-CNN applies the CNN to every
          <i>image region</i> in the original image. But SPP-Net applies the CNN
          once to produce the feature maps, and then applies
          <i>
            only the SPP layer and fully connected layers to each feature
            region.</i
          >
          This process is illustrated in the image below.
        </p>
        <div class="image-container">
          <img src="/images/object_detection/spp/spp-feature-maps.png" />
          <p>
            <i
              >This is Figure 5 from the SPP-Net paper, credit goes to the paper
              authors.</i
            >
          </p>
        </div>
        <p>
          Why is this important? Two reasons, the first is that by doing a
          spatial pyramid pool of the region, rather than simply flattening it,
          you get a better feature vector because it's considering multiple
          scales. Second, by running the pooling on feature map regions, instead
          of image regions, you only run the CNN once, which makes your network
          about 2000 times faster, not bad. (In practice, for various reasons,
          they saw a speed up of about 200).
        </p>
        <h3>Next Up</h3>
        <p>
          With that, we're ready to tackle Fast R-CNN. See you in the next post.
        </p>
        <p>
          <a
            href="https://www.harrysprojects.com/pages/object_detection/fastrcnn.html"
            >Next: Fast R-CNN</a
          >
        </p>
        <br />
        <div id="disqus_thread"></div>
        <script>
          var disqus_config = function () {
            this.page.url =
              "https://www.harrysprojects.com/pages/object_detection/spp.html";
            this.page.identifier = "spp";
          };

          (function () {
            // DON'T EDIT BELOW THIS LINE
            var d = document,
              s = d.createElement("script");
            s.src = "https://harrysprojects-com.disqus.com/embed.js";
            s.setAttribute("data-timestamp", +new Date());
            (d.head || d.body).appendChild(s);
          })();
        </script>
        <noscript
          >Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript"
            >comments powered by Disqus.</a
          ></noscript
        >

        <br />
        <br />
      </div>
    </div>
  </body>
  <footer></footer>
</html>
