<!doctype html>

<html lang="en">
    <head>
        <meta charset="utf-8">
         <!-- Review and updated March 2024
        canonical: yes
        navbar: yes
        images: yes
        disqus: yes
        analytics: yes
        accessible: yes
        description: yes
        title: yes
        keywords: yes    
        -->
        <title>EfficientNet-X</title>
        <meta name="description" content="An introduction and explanation to EfficientNet-X.">
        <meta name="author" content="Harry Turner">
        <meta name="keywords" content="learn, tutorial, paper,deep learning, machine learning, efficientnet, efficientnetx,
        datacenter accelerator, Neural Architecture Search, Latency Aware Compound Scaling, Compound Scaling, FLOPs, GPU, TPU, Latency, Throughput,
        roofline model, search space, space to depth, mbconv">
        <link rel="icon" type="image/ico" href="../../images/favicon.ico">
        <link href="../../styles/styles.css" type="text/css" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
        <link rel=“canonical” href=“https://www.harrysprojects.com/pages/miscellaneous/efficientnetx.html” />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../../js/Navbar.js" type="text/javascript" defer></script>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-X5NEKXRJE4"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
s
          gtag('config', 'G-X5NEKXRJE4');
        </script>
    </head>
    <body>
        <div class="page-container">

            <nav-bar></nav-bar>

            <div>
                <h1>Searching for Fast Model Families on Datacenter Accelerators</h1>
                <p><i>13th March 2021</i></p>
                <p><i>Reading time about 21 minutes.</i></p>
                <p><i>
                    This article is an introduction and explanation of the paper <a href="https://arxiv.org/pdf/2102.05610.pdf">Searching for Fast Model Families on Datacenter Accelerators</a> that was released by a team at Google in February 2021. We’ll explore why
                    the current state of the art efficient models like EfficientNet are not running as optimally as
                    they should be given the powerful hardware they’re deployed on, and more importantly, we’ll learn
                    what we can do about it.
                </i></p>
                <br>
                <h3>Introduction</h3>
                <p><i>How do you make a deep neural network run faster?</i></p>
                <p>
                    There are a few ways to think about this. Firstly, if your model does less computation, then it will
                    run faster simply because it has to do less work. In practice, this means <i>compressing</i> your
                    model. Unfortunately, to get good task performance you usually need lots of parameters,
                    which means more computation. There are tricks to reduce computation without compromising accuracy,
                    such as replacing all your convolutions with depthwise separable ones for example, but generally, those two
                    things are traded off against each other.
                </p>
                <p>
                    The other way to think about this is to realise that even if your model is compressed as much
                    as possible and doing very little computation if it's not running efficiently on the hardware then
                    you're still not doing as well as you could be. In other words, for the same <i>latency</i>,
                    you could get away with a bigger and more powerful model if only you could run it more efficiently.
                    Alternatively, you could have the smallest model in the world but if it's running at 10 FLOPs per
                    second you're going to be waiting a long time.
                </p>
                <p>
                    The second way to make your model run fast is therefore to deploy it efficiently, on efficient
                    hardware, like a GPU. There are frameworks out there that help you do this, such as
                    <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> from
                    NVIDIA, that build optimised versions of your model targetted at specific hardware. In practice,
                    this is a good option to pursue, if you can make it work for your model then you get a lot of
                    optimisation "for free".
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/tensorrt.png"/>
                    <p>
                        <i>TensorRT optimises trained neural network models to produce deployment-ready models.
                        Source: <a href="https://blog.tensorflow.org/2018/04/speed-up-tensorflow-inference-on-gpus-tensorRT.html">
                                TensorFlow Blog</a></i>
                    </p>
                </div>
                <p>
                    The paper <a href="https://arxiv.org/pdf/2102.05610.pdf">Searching for
                    Fast Model Families on Datacenter Accelerators</a> is mostly about this second approach. The authors
                    at Google noticed that recent state-of-the-art efficient
                    networks, like <a href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html">EfficientNet</a>,
                    may be doing less computation - i.e they are compressed. But they're
                    running <i>inefficiently</i> on dedicated hardware like V100 GPUs or Google TPUs. The trick to
                    faster models, they claim, is to create a new architecture family of models that is optimised
                    for running efficiently on dedicated hardware.
                </p>
                <p>
                    I wrote this article to introduce and explain the paper, and it is intended to be read before or
                    alongside it and follows the order of the paper almost exactly. It is organised as follows.
                </p>
                <p>
                    First up, a couple of definitions to ensure we're all talking about the same things. Next, I'll
                    introduce the roofline model of processor performance and show how it helps us think about
                    performance bottlenecks in modern processors. This will give us hints as to how to change our
                    models in order to improve performance.
                </p>
                <p>
                    After that, I'll introduce Neural Architecture Search and how it's used to systematically and
                    automatically search for high performing models. I'll introduce the idea of the search space and
                    the new building blocks that the authors add to this search space to find more efficient networks.
                </p>
                <p>
                    Next, I introduce the concept of a model family and explain how compound scaling is used to scale
                    up the base model to target different accuracy/latency targets. Then I introduce the Latency
                    Aware Compound Scaling method that takes latency into account when building the model family.
                </p>
                <p>
                    Finally, we'll take a quick look at the architecture of EfficientNet-X itself.
                </p>
                <p><i>
                    <b>Disclaimer:</b>
                    None of the ideas in this article are my own, I am bringing together information from several
                    sources to more clearly and intuitively explain the paper and its results. I use several
                    images to visually explain and clarify my points, any images shown that are not my
                    own are clearly marked with the original source.
                </i></p>

                <h3>Section 1 - Definitions</h3>
                <p>
                    <b>Latency.</b> The time required to perform some action, do some computation, or produce some result.
                    Latency is measured in units of time -- hours, minutes, seconds, nanoseconds, or clock periods.
                    <a href="https://community.cadence.com/cadence_blogs_8/b/sd/posts/understanding-latency-vs-throughput">Source.</a>
                </p>
                <p>
                    <b>Throughput.</b> Throughput is the number of such actions executed or results produced per unit
                    of time. Measured in units of whatever is being produced (cars, motorcycles, iterations) per unit
                    of time. <a href="https://community.cadence.com/cadence_blogs_8/b/sd/posts/understanding-latency-vs-throughput">Source.</a>
                </p>
                <p>
                    <b>FLOPs.</b> A FLOP is a floating-point operation and is essentially a mathematical
                    operation (i.e +, *, ...) that a computer performs on floating-point numbers. FLOPS/s is the
                    number of such operations performed per second, also known as the computation rate. Later in this
                    article, I'll refer to a model as having some number of FLOPs, this is the amount of computation it
                    has to do to achieve a result.
                </p>
                <p>
                    <b>GPU.</b> Graphics Processing Unit, a processor optimised for throughput rather than latency,
                    originally designed for computer graphics but because the underlying computations are the same, are
                    also widely used to train and execute deep neural networks.
                </p>
                <p>
                    <b>TPU.</b> Tensor Processing Unit, a processor developed and built by Google specifically for
                    deep neural network learning and inference. They're available for third party use on its cloud
                    infrastructure.
                </p>
                <p>
                    <b>CPU.</b> Central Processing Unit, a processor optimised for latency.
                </p>
                <p>
                    <b>Datacenter Accelerator.</b> Hardware designed and used for processing data in datacenters. In
                    the context of this paper, it refers to both GPUs and TPUs.
                </p>
                <p>
                    <b>EfficientNet.</b> A <a href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html">
                    model architecture family</a> released in 2019 by Google that targetted low FLOPs.
                </p>
                <p>
                    <b>EfficientNet-X.</b> The model architecture family released in this paper that targets low latency
                    for specific hardware. Not to be confused with EfficientNet. It can be considered an evolution of
                    EfficientNet for the purposes of this article.
                </p>

                <h3>Section 2 - Why FLOPs and Latency do not Correlate</h3>

                <p>
                    Modern processors, like GPUs, are getting faster all the time. Recent progress is mostly due to
                    the introduction of special features such as matrix-multiply-accumulate units, or tensor cores.
                    These are specialised bits of hardware inside the GPU that are dedicated to running the kinds of operations that neural
                    networks need, just like the ray-tracing cores on the latest GPUS are dedicated to running ray
                    tracing operations for video games.
                </p>
                <p>
                    At the same time, modern neural networks are getting more complex, and more computationally intensive. You
                    would hope that the increase in computational demand would be matched by the increase in computation
                    capability offered by the processor. But in practice, it's not as simple as that. Recent empirical
                    results have shown that many modern neural network architectures have what the authors refer to as
                    a <i>FLOPs-latency non-proportionality</i> which in English means they're not running as fast as they
                    should be given the powerful hardware they're running on.
                </p>
                <p>
                    The authors set out to understand why modern complex architectures weren't running as efficiently
                    as would be expected on these new powerful processors. To do that, they build a
                    performance model and then analyse it. To understand the implications of this performance model,
                    we need to take a short detour to talk about the roofline model.
                </p>

                <h5><b>The Roofline Model</b></h5>
                <p>
                    A roofline model is a form of <i>bound and bottleneck analysis</i>. Its purpose is to
                    be an easy-to-understand model that provides valuable insight into the primary factors affecting
                    the performance of computer systems. It doesn't need to be perfect, just useful. There are two
                    dimensions for thinking about processor performance.
                </p>
                <p>
                    The first is called <i>Peak Performance</i> which is essentially <i>how fast can your processor
                    crunch numbers if there was nothing else blocking it.</i> It's raw processor performance and is
                    measured in floating-point operations per second, or FLOPs/s. Ideally, we want our processor to be
                    hitting it's maximum processing speed as much as possible so that we're using it efficiently. This
                    is a hard limit on performance, you can't go faster than the processor can handle. The way to
                    improve this limit is to buy a faster one.
                </p>
                <p>
                    The second dimension is called <i>Memory Performance</i>. All processors need to load data from
                    memory before they do computation on it, and loading data from memory is <i>slow</i>. The speed at
                    which data can be read from memory depends on where it is, data stored in registers takes 1 CPU cycle,
                    data stored in the L3 cache takes roughly 75 CPU cycles, and data stored in RAM takes roughly 200
                    CPU cycles. For comparison, a single floating point operation takes about 1 CPU cycle. Even if we
                    have the fastest processor in the world, if it has to load a whole lot of data for every computation
                    it does, then it's going to be twiddling it's thumbs most of the time waiting for that data to arrive.
                    Ideally, we want to load a piece of data and then do a whole bunch of computation on it before we
                    load the next piece.
                </p>
                <p>
                    This idea of doing as much computation on a piece of loaded data is important and it has a name. It's
                    called <i>Operational Intensity</i> and it's measured in FLOPs per Byte of data loaded. Intuitively
                    you can think of it as how <i>efficient</i> we're being when doing computation on our loaded data.
                    operational intensity is a property <i>of the algorithm</i>, it's something that the programmer can control, or
                    at least the compiler. <i>Peak Performance</i> and <i>Memory Performance</i> are properties of the
                    processor, you cannot optimise beyond those hard limits without buying a different processor.
                </p>
                <p>
                    We're now in a position to understand the roofline model, which relates the two performance bottlenecks
                    described above. Those two hard processor limits become hard bounds on our performance, they tell us
                    the <i>achievable</i> performance and are represented by the bold black lines on the plot below. The
                    reason it's called the roofline mode is because.. well.. it sort of looks like a roof.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/roofline.png"/>
                    <p><i>The Roofline Model. Source <a href="https://hpc-wiki.info/hpc/Performance_model">HPC WIKI</a></i></p>
                </div>
                <p>

                    On the y-axis, we have performance in (Giga)FLOPs per second, which measures our rate of computation. On the
                    x-axis, we have operational intensity (also known as computational intensity), which measures how computationally efficient our algorithms are
                    with respect to memory access. The Peak Performance limit is a horizontal line at the maximum computation
                    rate, we cannot go past this. The sloped line is a bit more subtle, it says that the limiting factor
                    is now the memory bandwidth, <i>not processor speed</i>. If your algorithm has a low operational
                    intensity, then it's hitting the memory too often and that becomes the limiting factor. The point
                    at which the two lines meet is called the ridge-point, and intuitively it tells you the minimum
                    operational intensity required to achieve maximum performance.
                </p>
                <p>
                    There's much more to say about the roofline model and I provide references at the end of this article,
                    but the main takeaway for us is that we want to run our algorithms in the Peak Performance saturation
                    region, that's where we're using the processor most efficiently. To do that, our algorithms need
                    a high enough operational intensity to get past the ridge-point, otherwise, we're memory limited.
                </p>
                <p>
                    Okay, back to neural networks. The image below is Figure 2 copied from the paper, it shows the
                    roofline model for several DC Accelerators and the position of several modern neural network
                    architectures.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/roofline_fig_2.png"/>
                    <p><i>Figure 2 from the paper. Source <a href="https://arxiv.org/pdf/2102.05610.pdf">Paper</a></i></p>
                </div>
                <p>
                    Now that you have a good understanding of the roofline model, it should be easy to spot the problem.
                    Modern architectures are in the <i>memory-bound</i> region of the roofline, this means that
                    despite them having fewer FLOPs, they are not using the processor as efficiently as they could be.
                    Specifically, they have low <i>operational intensity</i>. To improve the situation we need to
                    improve the operational intensity of our architectures.
                </p>
                <p>
                    There are two more insights I want to point out before moving on that I think are useful. The
                    first is, not only are the architecture's memory bound, but <i>they don't actually lie on the boundary.</i>
                    This has significance, it means that there is yet another performance limit affecting the models that isn't
                    shown on this particular roofline plot.  The resources I provide at the end of this article go into more detail on
                    something called <i>Ceilings</i> which explains this in more detail if you're interested.
                </p>
                <p>
                    The second insight is to notice that the shape of the roofline for modern DC Accelerators like GPUs
                    and TPUs tends to have the following properties. The gradient of the memory-bound region typically
                    doesn't change, notice how it's similar for the CPU. But the maximum Peak Performance increases a lot,
                    mainly due to the dedicated specialised hardware mentioned earlier. This has the significant effect of
                    pushing the ridge-point to the right. This means that just because you're using a new faster processor,
                    it doesn't automatically mean your networks will run faster, because <i>you may also need to increase
                    the operational intensity of those networks to take advantage of the new processor.</i> That's an
                    important takeaway.
                </p>
                <p>
                    Our analysis of the roofline performance for modern architectures on modern hardware tells us that
                    we need to improve the operational intensity of our models. In practice, this means that we need to
                    do more computation for the same piece of loaded data. The way we do this is to do more parallelism.
                </p>
                <p>
                    At this point, we've reframed our problem. We know we want faster networks and we know what's
                    holding them back. The new problem then, is how can we design new architectures that better support
                    parallel computation? How do we piece together the building blocks of a network in a way that lets it run
                    efficiently on modern hardware? Enter Neural Architecture Search...
                </p>

                <h3>Section 3 - A Better NAS Search Space</h3>
                <p>
                    Neural Architecture Search, or NAS, is a method of systematically and automatically finding
                    model architectures by searching through a search space of building blocks. NAS can typically be
                    characterised with three components.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/nas.png"/>
                    <p><i>The main components of Neural Architecture Search.</i></p>
                </div>
                <p>
                    <b>Search Space.</b> Defines the set of building blocks out of which you'll build your network.
                    Things like convolution layers, fully connected layers, and max pooling. These building blocks also
                    define how they should be connected together to produce a valid network.
                </p>
                <p>
                    <b>Search Algorithm.</b> The optimisation algorithm generates the architecture candidates
                    using the pool of building blocks.
                </p>
                <p>
                    <b>Evaluation Strategy.</b> The method for testing a candidate architecture. This could be as simple
                    as training it on a dataset and evaluating its performance on a test set, but NAS may end up
                    proposing hundreds or thousands of candidates and this could get expensive quickly! Alternative
                    evaluation strategies include doing things like estimating performance or even predicting it.
                </p>
                <p>
                    I don't want to go too much into NAS here, it's a big field and we'll get lost down a rabbit hole.
                    I've linked some resources at the end of the article for those interested. The important takeaway
                    is to recognise that we have an optimisation problem. Let me repeat the problem that I restated
                    above. <i>How do we piece together the building blocks of a network in a way that lets it run
                    efficiently on modern hardware?</i> How is this an optimisation problem? Firstly we have a
                    representation of a candidate architecture, defined by the set of building blocks arranged together
                    to produce a network, and we have a target value we want to optimise, which is network accuracy whilst
                    minimising latency. Those two components plus the optimiser itself make a well-formed optimisation
                    problem.
                    For our current purposes, NAS is simply a set of tools and methods that help us solve that optimisation
                    problem. NAS brings to the table the concept of a search space (and a diverse set of building blocks
                    for building networks already exists), a set of search algorithms for searching through that space,
                    and a set of evaluation methods for quickly estimating or predicting the performance of the candidates.
                </p>
                <p>
                    The point I'm making is that we don't need to focus much on the latter two points, we can consider
                    them solved for now, this is not a paper on NAS. What we <i>do</i> need to pay attention to however
                    is the search space of building blocks, because that defines what kind of networks we can build. If
                    we want to build networks that are optimised for running efficiently on modern hardware, then we
                    need to add these kinds of operations into the search space. That's exactly what Section 3 of the
                    paper is all about.
                </p>
                <p>
                    In the following four sections, I will introduce four new kinds of building blocks that the authors
                    added to the search space, and I'll comment on why each was added.
                </p>
                <h5><b>Space to Depth</b></h5>
                <p>
                    Very simply, a space to depth operation moves values from the height and width dimensions into the
                    depth dimension, without changing the overall size of the tensor. Therefore it is used for
                    <i>lossless dimensionality reduction</i>.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/spacetodepth.png"/>
                    <p><i>Space to Depth Operation</i></p>
                </div>
                <p>
                    It often replaces pooling operations. This operation is
                    useful for building more efficient models because tensors with larger depths and batches can be
                    more easily parallelised. You can think of this operation as converting a tensor into a shape
                    that can more easily be parallelised without losing any information.
                </p>
                <p>
                    Tensorflow has <a href="https://www.tensorflow.org/api_docs/python/tf/nn/space_to_depth">an operation</a>
                    to do exactly this. However, the authors recreate their own operation
                    using a convolution. The first reason is that convolutions have higher operational intensity and
                    execution efficiency. The second reason is that because convolutions perform computation, they add
                    to the models capacity.
                </p>
                <h5><b>Space to Batch</b></h5>
                <p>
                    The argument for space to batch operations is identical to that for space to depth. The only
                    difference is that it cannot be done with a convolution so the authors use
                    <a href="https://www.tensorflow.org/api_docs/python/tf/space_to_batch">operations provided
                        by common frameworks</a>. Unfortunately, these operations are memory-intensive copy-reshape operations.
                </p>
                <h5><b>Fused Convolution Structures</b></h5>
                <p>
                    A convolution structure is a set of convolutions and other layers organised in an arrangement that
                    achieves a specific objective. They are usually treated as building blocks in their own right. The
                    image below is Figure 2 from the paper <a href="https://arxiv.org/pdf/1801.04381v4.pdf">MobileNetV2:
                    Inverted Residuals and Linear Bottlenecks</a>. These are examples of convolution structures <i>
                    that could all be swapped out with regular convolutions</i>.
                </i>
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/convolutionstructures.png"/>
                    <p><i>Examples of Convolutional Structures. Source: <a href="https://arxiv.org/pdf/1801.04381v4.pdf">MobileNetV2:
                    Inverted Residuals and Linear Bottlenecks</a></i></p>
                </div>
                <p>
                    In Section 3.2 of our paper, the authors talk about an MBConv block, which is also called an Inverted
                    Residual Block, which comes from the paper linked above. The block first starts with a 1x1
                    convolution to <i>expand</i> the number of channels. Then it does a depthwise separable convolution
                    then it does a final 1x1 convolution to <i>project</i> the channels back down to the required number.
                    Note there are also activations, batch normalisation, and skip connections (which are cut off slightly in the image below).
                    The block itself has quite a large capacity for learning, whilst being much more efficient in its
                    computations.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/mbconv.png"/>
                    <p><i>Inverted Residual Block (MBConv). Source: <a href="https://arxiv.org/pdf/1801.04381v4.pdf">MobileNetV2:
                    Inverted Residuals and Linear Bottlenecks</a></i></p>
                </div>
                <p>
                    Note that this block <i>is already in the NAS search space</i>, so in our paper, the authors recognise
                    that the default MBConv block has <i>low FLOPs</i> which is good for speed, but unfortunately it has
                    <i>a low operational intensity</i> which is bad for speed. Recall that the issue we identified with
                    modern architectures is their low operational intensity. To fix this, the authors propose a <i>
                    fused version of MBConv</i>. I won't draw it here but a fused variant simply combines the depthwise
                    convolutions with the expansion or projection layer and does it with a vanilla convolution. The
                    fused MBConv has <i>high operational intensity</i> but now has <i>higher FLOPs</i>.
                </p>
                <p>
                    So which is better, the traditional MBConv or the fused version? The authors note that there is too much complexity and dependency on the surrounding network and
                    other factors to compute it manually, so they simply pop it into the search space and let NAS figure
                    it out.
                </p>
                <h5><b>Block-Wise Searchable Activation Functions</b></h5>
                <p>
                    All activation functions have low operational intensity, and on most GPUs and TPUs most are
                    therefore memory bound. The most important optimisation is fusing with the preceding convolution,
                    which hides the performance cost as it's done in parallel with the convolution. Very simply the
                    authors augment the search space with activation functions including ReLU and Swish. They apply
                    them at the block level which means all layers within a block have the same function but different
                    blocks can have different functions.
                </p>
                <h5><b>So What are we Optimising?</b></h5>
                <p>
                    So that's the search space covered. Recall that NAS combines blocks from the search space to produce
                    candidate architectures and tries to find the architecture that maximises an objective expression.
                     Let's visit that in a bit more detail. The multi-objective reward
                    is presented below and it combines accuracy and latency into one expression. We give a higher reward
                    if NAS can maximise accuracy <i>and</i> achieve a latency below \( \color{#e83e8c} TargetLatency \).
                    The \( \color{#e83e8c} w \) parameter trades off between accuracy and latency and is typically a small negative
                    number.
                </p>
                <div class="image-container">
                    </br>
                    <span style="font-size: 24px">\( \color{#e83e8c} Accuracy(m).\left [ \frac{Latency(m)}{TargetLatency} \right ]^{w} \)</span>
                    <p><i>NAS Multi-Objective Reward from the paper. Source: <a href="https://arxiv.org/pdf/2102.05610.pdf">Paper</a></i></p>
                </div>
                <p>
                    NAS will maximise this expression. To see why it has the form that it does, first notice that increasing
                    the accuracy will directly maximise the first term. Secondly, because \( \color{#e83e8c} w \) is negative,
                    we can maximise the second term by minimising latency. Specifically, when the latency is equal
                    to the target, the second term evaluates to 1, when it's less than the target the second term is
                    larger than 1 and vice versa.
                </p>
                <p>
                    That wraps up this section on NAS. I will not go into any more details on how NAS actually works
                    or how the optimal architecture is found, as it's beyond the scope of the paper. From this point
                    on I will assume that we've now discovered an architecture that is optimised for high accuracy
                    and low latency. I'll refer to this model as EfficientNet-X-B0 - our base model. Now we need to
                    scale it up to produce the EfficientNet-X model family. We'll do this with Latency Aware Compound Scaling.
                </p>
                <h3>Section 4 - Latency Aware Compound Scaling (LACS)</h3>
                <p>
                    Scaling a neural network means taking a base model and then making it bigger by either increasing the
                    number of layers (making it deeper), increasing the number of channels in each layer (making it wider), or
                    increasing the input image size (increasing the resolution). All three of those will
                    increase network capacity and therefore task performance, it will also increase FLOPs.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/compoundscaling.png"/>
                    <p><i>Figure 2 from EfficientNet paper. Source: <a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet:
                    Rethinking Model Scaling for Convolutional Neural Networks</a></i></p>
                </div>
                <p>
                    Compound Scaling means scaling up those parameters at the same time in a principled way. This makes a
                    lot of sense. If the input image is bigger, then the network probably needs more layers to increase
                    the receptive field and more channels to capture more fine-grained patterns on the bigger image. By
                    scaling in this way we can create what is called a model family, where all models share the same basic
                    architecture but each targets a different accuracy/FLOPs tradeoff. This idea was introduced in the
                    original EfficientNet paper and used to create the family of EfficientNet models. <i>This is not to be
                    confused with EfficientNet-X introduced in <u>this</u> paper.</i> Below I have copied in the table from the <i>EfficientNet</i> paper that
                    summarises the eight models in the family, notice the increasing accuracy and increasing FLOPs as
                    it moves from B0 to B7.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/efficientnetfamily.png"/>
                    <p><i>Table 2 from EfficientNet paper. Source: <a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet:
                    Rethinking Model Scaling for Convolutional Neural Networks</a></i></p>
                </div>
                <p>
                    A natural question is why scale in the first place? NAS is already finding good architectures,
                    can't it just find an architecture for each accuracy/FLOPs target? Theoretically yes, in practice no. It's
                    just too expensive. Directly searching for the entire EfficientNet family is about 100x times more
                    expensive than searching for EfficientNet-B0. The way it's typically done is NAS finds what's
                    called a base model, in this case, EfficientNet-B0, and then model scaling is used to scale that up.
                    In practice, it also means the models are easier to implement. When I implemented EfficientNet
                    with my colleagues, we defined the basic architecture once and then used only for-loops to build
                    the different architectures based on the set of configurations for each model.
                </p>
                <p>
                    Just to orientate you, recall that at the end of the section on NAS, we had found a base model
                    called EfficientNet-X-B0, a <i>different</i> architecture to the EfficientNet-B0 mentioned above.
                    In this next few paragraphs, I will introduce Compound Scaling in general and then we'll apply
                    it to our EfficientNet-X-B0 base model to produce the EfficientNet-X family which has been our mission
                    all along.
                </p>
                <h5><b>Compound Scaling</b></h5>
                <p>
                    Compound Scaling is another optimisation problem. NAS was all about finding an optimal architecture.
                    Compound Scaling is all about optimising <i>how we scale up that architecture</i>. Specifically, we take
                    our baseline architecture (found through NAS)
                    and fix the type of the individual layers. The only variables we are allowed to play with are depth,
                    width, and resolution. Increasing depth means increasing the number of layers in each stage
                    (keeping the type of each layer fixed), increasing width means increasing the number of channels in each layer,
                    and increasing resolution means increasing the size of the input image. Technically we could change
                    these three values <i>for each stage in the network</i>, but that makes the design space too large so
                    we constrain the design space such that all stages use the same values. For example, increasing the
                    width increases the number of channels in <i>each layer</i> uniformly. This makes the problem more
                    tractable, whilst still leaving us with a reasonably flexible design space. We can play around with
                    these three parameters and using the same baseline architecture we can create different
                    networks that have different accuracies and FLOPs.
                </p>
                <p>
                    Compound Scaling says we should scale the depth, width, and resolution together. In practice, that
                    means we couple them together like this.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/eq3.png"/>
                    <p><i>Equation 3 from EfficientNet paper. Source: <a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet:
                    Rethinking Model Scaling for Convolutional Neural Networks</a></i></p>
                </div>
                <p>
                    \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \) are
                    constants that we'll determine shortly. Intuitively, \( \color{#e83e8c} \phi \) is a user-specified
                    coefficient that controls how many more resources are available for model scaling, whilst
                    \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \)
                    specify how to assign these extra resources to network width, depth, and resolution respectively.
                    Notice the constraint that the product of the three parameters must equal 2, this is an arbitrary
                    constraint that is set to make scaling more intuitive. Increasing \( \color{#e83e8c} \phi \)
                    will increase the FLOPs of the network by \( \color{#e83e8c} 2^{\phi} \), and more concretely, increasing
                    \( \color{#e83e8c} \phi \) by 1, doubles the FLOPs.
                </p>
                <p>
                    Notice that we have four free parameters, the individual constants \( \color{#e83e8c} \alpha \),
                    \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \) and the scaling
                    parameter \( \color{#e83e8c} \phi \). Ultimately we will want to fix the constants and change only
                    the scaling parameter, but to do that we need to choose optimal values for those constants first.
                    The way we do that is with a two-step optimisation.
                    First, we fix the coefficient \( \color{#e83e8c} \phi \) at 1, which implicitly assumes we've got twice the resources available because
                    of the constraint that FLOPs increase by \( \color{#e83e8c} 2^{\phi} \). We then take our baseline
                    architecture and we play with the values of \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \),
                    and \( \color{#e83e8c} \gamma \) to get the best accuracy we can achieve. Then we fix those values as constant.
                </p>
                <p>
                    Once we've found the optimal constants, we only have one free parameter to play with, \( \color{#e83e8c} \phi \),
                    which lets us scale up the network
                    to produce the eight different models (or as many as we want).
                </p>
                <h5><b>Latency Aware Compound Scaling</b></h5>
                <p>
                    What I've just described is Compound Scaling as introduced in the EfficientNet paper. Our paper
                    talks instead about something called <i>Latency Aware</i> Compound Scaling, or LACS. The only
                    difference between the two is the target objective that we're maximising. Compound Scaling simply
                    maximises accuracy, whilst LACS maximises the multi-objective of accuracy and latency. Recall that
                    the first step was to fix \( \color{#e83e8c} \phi \) and vary the
                    \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \)
                    parameters. Well the LACS optimisation problem is of the following form and can be read as:
                    vary \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \)
                    which give you \( \color{#e83e8c} d \), \( \color{#e83e8c} w \), and \( \color{#e83e8c} r \)
                    respectively, maximise the accuracy and latency of a model whilst constraining latency to the
                    target latency.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/eq4.png"/>
                    <p><i>Equation 4 from the paper. Source: <a href="https://arxiv.org/pdf/2102.05610.pdf">Paper</a></i></p>
                </div>
                <p>
                    That's a bit of a mouthful but it's exactly the same as step 1 that I described earlier, we
                    fix \( \color{#e83e8c} \phi \) and vary \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \),
                    and \( \color{#e83e8c} \gamma \) except now we maximise both accuracy <i>and</i> latency. This will give
                    us values of \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \)
                    that we then fix as constants. After that, we scale up \( \color{#e83e8c} \phi \) as usual to generate the family of models.
                </p>
                <p>
                    At this point, we can return to our baseline model, EfficientNet-X-B0. We fix \( \color{#e83e8c} \phi \)
                    at 1 and optimise \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \)
                    to maximise the accuracy and latency. We fix the resulting values as constants
                    and then we increase \( \color{#e83e8c} \phi \) to produce eight different models and there we have it,
                    the EfficientNet-X model family which we'll see shortly.
                </p>
                <p>
                    Before we leave this section there are a few things I want to point out. The first is that the authors
                    do the above optimisation for both GPU and TPU, finding slightly different values of
                    \( \color{#e83e8c} \alpha \), \( \color{#e83e8c} \beta \), and \( \color{#e83e8c} \gamma \) for
                    each one. They also find that alpha comes out to be larger when using LACS instead of normal
                    Compound Scaling, which means that scaling models for low latency means depth should grow faster
                    than width or resolution. Below I have copied in Table 2 from the paper that summarises these differences.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/aby.png"/>
                    <p><i>Table 2 from the paper. Source: <a href="https://arxiv.org/pdf/2102.05610.pdf">Paper</a></i></p>
                </div>
                <p>
                    This can be explained intuitively by noting that the original compound scaling coefficient \( \color{#e83e8c} \phi \)
                    represented the extra FLOPs available for model scaling, in the new latency aware scaling,
                    it now represents the latency budget - i.e it's capturing a different kind of <i>resource</i> that is
                    more appropriate for the hardware.
                </p>
                <p>
                    Lastly, I want to clarify a possible point of confusion. The objective we're maximising in LACS is a
                    multi-objective of accuracy and latency. This has the same form as the multi-objective in NAS, <i>but
                    they are not constrained to be the same</i>. Specifically, they are two separate optimisation problems.
                </p>
                <h3>Section 5 - EfficientNet-X</h3>
                <p>
                    And we're done. We've found a family of EfficientNet-X models that have been scaled up from B0 to B7.
                    Table 3 from the paper summarises this brilliantly, and what's nice is that it also compares to the
                    original EfficientNet family as well.
                </p>
                <div class="image-container">
                    <img src="../../images/miscellaneous/efficientnetx/efficientnetxfamily.png"/>
                    <p><i>Table 3 from the paper. Source: <a href="https://arxiv.org/pdf/2102.05610.pdff">Paper</a></i></p>
                </div>
                <h3>Conclusion</h3>
                <p>
                    That concludes this article. There is a lot more detail in the paper and I deliberately haven't
                    covered everything here because I didn't want this article to get too lengthy. For example, the
                    paper goes into detail into the differences between EfficientNet-X on TPU vs GPU, it
                    explains a bit more about how they carried out NAS, presents several ablation studies, and more.
                </p>
                <p>
                    My intention with this article was to provide enough introduction and background that you should be
                    able to easily pick up the paper and follow it through to the end. I hope I've met that goal.
                </p>
                <p>
                    I'm always looking for feedback on how to improve my writing, specifically my ability to communicate
                    technical ideas as simply as possible. If you have any feedback for me I'd be very grateful, and of
                    course, if you spot any mistakes, please do let me know.
                </p>

                <h3>References</h3>
                <p><a href="https://arxiv.org/pdf/2102.05610.pdf">Searching for Fast Model Families on Datacenter Accelerators</a></p>
                <p><a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></p>
                <p><a href="https://community.cadence.com/cadence_blogs_8/b/sd/posts/understanding-latency-vs-throughput">Understanding Latency vs Throughput</a></p>
                <p><a href="https://dando18.github.io/posts/2020/04/02/roofline-model">Roofline-Model</a></p>
                <p><a href="https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf#">Roofline - Berkeley</a></p>
                <p><a href="https://www.tensorflow.org/api_docs/python/tf/nn/space_to_depth">Space to Depth</a></p>
                <p><a href="https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html">Neural Architecture Search</a></p>
                <p><a href="https://arxiv.org/pdf/2005.11074.pdf">An Introduction to Neural Architecture Search for Convolutional Neural Networks</a></p>

                <br />
                <div id="disqus_thread"></div>
                <script>
                    var disqus_config = function () {
                    this.page.url = "https://www.harrysprojects.com/pages/miscellaneous/efficientnetx.html";
                    this.page.identifier = "efficientnetx";
                    };

                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://harrysprojects-com.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })();
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

                <br />
                <br />
            </div>

        </div>
    </body>
    <footer>
    </footer>
</html>